1912.06612v1 [cs.AI] 13 Dec 2019

arXiv

From Shallow to Deep Interactions Between Knowledge
Representation, Reasoning and Machine Learning

Kay R. Amel!
GDR Aspects Formels et Algorithmiques de I’Intelligence Artificielle
CNRS

December 16, 2019

Abstract

This paper proposes a tentative and original survey of meeting points between Knowledge Repre-
sentation and Reasoning (KRR) and Machine Learning (ML), two areas which have been developing
quite separately in the last three decades. Some common concerns are identified and discussed such
as the types of used representation, the roles of knowledge and data, the lack or the excess of infor-
mation, or the need for explanations and causal understanding. Then some methodologies combining
reasoning and learning are reviewed (such as inductive logic programming, neuro-symbolic reasoning,
formal concept analysis, rule-based representations and ML, uncertainty in ML, or case-based reasoning
and analogical reasoning), before discussing examples of synergies between KRR and ML (including
topics such as belief functions on regression, EM algorithm versus revision, the semantic description of
vector representations, the combination of deep learning with high level inference, knowledge graph
completion, declarative frameworks for data mining, or preferences and recommendation). This paper
is the first step of a work in progress aiming at a better mutual understanding of research in KRR and
ML, and how they could cooperate.

1 Introduction

Reasoning and learning are two basic concerns at the core of Artificial Intelligence (AI). In the last three
decades, Knowledge Representation and Reasoning (KRR) on the one hand, and Machine Learning (ML)
on the other hand, have been considerably developed and have specialised themselves in a large number
of dedicated sub-fields. These technical developments and specialisations, while they were strengthening
the respective corpora of methods in KRR and in ML, also contributed to an almost complete separation of
the lines of research in these two areas, making many researchers on one side largely ignorant of what is
going on, on the other side.

This state of affairs is also somewhat relying on general, overly simplistic, dichotomies that suggest
there exists a large gap between KRR and ML: KRR deals with knowledge, ML handles data; KRR privileges
symbolic, discrete approaches, while numerical methods dominate ML. Even if such a rough picture points
out facts that cannot be fully denied, it is also misleading, as for instance KRR can deal with data as
well [Pra16] (e.g., in formal concept analysis) and ML approaches may rely on symbolic knowledge (e.g.,
in inductive logic programming). Indeed, the frontier between the two fields is actually much blurrier
than it appears, as both are involved in approaches such as Bayesian networks, or case-based reasoning
and analogical reasoning, and they share important concerns such as uncertainty representation (e.g.,
probabilistic or possibilistic models, belief functions, imprecise probability-based approaches).

1Kay R. Amel is the pen name of the working group “Apprentissage et Raisonnement” of the GDR (“Groupement De Recherche”)
named “Aspects Formels et Algorithmiques de l’Intelligence Artificielle”, CNRS, France (https: //www.gdria.fr/presentation/). The
contributors to this paper include: Zied Bouraoui (CRIL, Lens, Fy, bouraoui@cril.fr), Antoine Cornuéjols (AgroParisTech, Paris,Fr,
antoine.cornuejols@agroparistech.fr), Thierry Denceux (Heudiasyc, Compiégne, Fr, thierry.denoeux@utc.fr), Sébastien Destercke
(Heudiasyc, Compiégne, Fr, sebastien.destercke@hds.utc.fr), Didier Dubois (IRIT, Toulouse, Fr, dubois@irit.fr), Romain Guillaume
CIRIT, Toulouse, Fr, Romain.Guillaume@irit.fr), Joao Marques-Silva (ANITI, Toulouse, Fr, joao.marques-silva@univ-toulouse.fr),
Jéré6me Mengin (IRIT, Toulouse, Fr, Jerome.Mengin@irit.fr), Henri Prade (IRIT, Toulouse, Fr, prade@irit.fr), Steven Schockaert
(School of Computer Science and Informatics, Cardiff, UK, SchockaertS$1@cardiff.ac.uk), Mathieu Serrurier (IRIT, Toulouse, Fr,
mathieu.serrurier@gmail.com), Christel Vrain (LIFO, Orléans,Fr, Christel. Vrain@univ-orleans.fr).
These remarks already suggest that KRR and ML may have more in common than one might think at
first glance. In that respect, it is also important to remember that the human mind is able to perform both
reasoning and learning tasks with many interactions between these two types of activity. In fact, from
the very beginning of the AI history, both reasoning and learning tasks have been considered, but not by
the same researchers; see, e.g., [DP19]. So, especially if the ultimate goal of AI is to have machines that
perform tasks handled by the human mind, it might be natural and useful to increase the cooperation
between KRR and ML.

The intended goal pursued in this work is to start and construct an inventory of common concerns
in KRR and ML, of methodologies combining reasoning principles and learning, of examples of KRR/ML
synergies. Yet, this paper is not an overview of the main issues in KRR crossed with an overview of the
main issues in ML, trying to identify when they meet. Doing so would lead to a huge and endless survey
since providing a survey of methodologies and tools for KRR alone, or for ML alone would be already a
colossal task”. In the following we rather try to identify a collection of meeting points between KRR and
ML. Since it is a work in progress, we do not expect to reach any form of exhaustiveness, and even some
important topics may remain absent from the document at this stage.

The paper is not either an introductory paper to KRR and/or ML. It is rather intended for readers who
are quite familiar with either KRR or ML, and who are curious about the other field. It aims in the long
range at contributing to a better mutual understanding of the two communities, and maybe to identify
some synergies worth of further research combining KRR and ML.

2 Common Concerns

In order to suggest and illustrate differences and also similarities between KRR and ML, let us start with
the simple example of a classification or recommendation-like task ,such as, e.g., associating the profile of
a candidate (in terms of skills, tastes) with possible activities suitable for him/her in a vocational guidance
system. Such a problem may be envisioned in different manners. On the one hand, one may think of it in
terms of a rule-based system relying on some expertise (where rules may be pervaded with uncertainty),
or on the other hand in terms of machine learning by exploiting a collection of data (here pertaining to
past cases in career guidance).

It is worth noticing that beyond the differences of types of representation that are used in both kinds of
approach (e.g., conditional tables for uncertainty assessment vs. weights in a neural net), there are some
noticeable similarities between (graphical) structures that can be associated with a rule-based reasoning
device, handling uncertainty (or an information fusion process) and with a neural net. This remark suggests
that, beyond differences in perspective, there is some structural resemblance between the two types of
process. This resemblance has been investigated recently in detail in the setting of belief function theory
[Den19], but an example may also be found in an older work on a possibilistic (max-min) matrix calculus
devoted to explainability (where each matrix represents a rule) [FP89].

Beyond this kind of parallel, it is clear that KRR and ML have common concerns. This section gives
an overview of the main ones regarding the representation issues, the complexity, the role of knowledge,
the handling of lack of information, or information in excess, uncertainty, and last but not least regarding
causality and explanation. Each subsection below tries to follow the same basic structure, by each time
providing i) the KRR view, ii) the ML view, and iii) some synthesis and discussion.

2.1 Types of Representation

In KRR, as suggested by the name, the main representations issues concern the representation of pieces
of knowledge (rather than data). The large variety of real world information has led to a number of
logical formalisms ranging from classical logic (especially proportional and first order) to modal logics (for
dealing with e.g., time, deontic, or epistemic notions) and to non classical logics for handling commonsense
reasoning.

?Nevertheless the interested reader is referred to appropriate chapters in [MPP19] or to monographs such as [BLO4, Bar10,
Hal17, SSBD14, MRT18].
The representation may use different formats, directed or undirected: sets of if-then rules, or sets of
logical formulas. A rule “if A then B” is a 3-valued entity (as first noticed in [DF36]), since it induces a
partition between its set of examples, its set of counterexamples and the set of items for which the rule is
irrelevant (i.e., when A is false). So a rule strongly departs from its apparent logical counterpart in terms
of material implication A — B (which is indeed non-directed, since equivalent —B — =A). This discrepancy
can be also observed in the probabilistic setting, since Prob(B|A) 4 Prob(A — B) in general. Rules may
hold up to (implicit) exceptions (see subsection 2.3).

Knowledge may be pervaded with uncertainty, which can be handled in different settings, in terms of
probability, possibility, belief functions, or imprecise probabilities (see subsection 2.3). In all of these cases,
a joint distribution can be decomposed in sub-distributions laying bare some form of conditional indepen-
dence relations, with a graphical counterpart; the prototypical graphical models in each representation are
respectively Bayesian networks (probabilistic), possibilistic networks, credal networks (imprecise proba-
bilities [Coz00]) or valuation-based systems (belief functions). Conceptual graphs [Sow84, CM09] offer a
graph representation for logic, especially for ontologies/description logics.

The main goal of KRR is to develop sound and (as far as possible complete) inference mechanisms to
draw conclusions from generic knowledge and factual data, in a given representation setting [HFMVO3,
Hal17]. The mathematical tools underlying KRR are those of logic and uncertainty theories, and more
generally discrete mathematics. An important issue in KRR is to find good compromises between the
expressivity of the representation and the computational tractability for inferring the conclusions of interest
from it [LB87]. This concern is especially at work with description logics that are bound to use tractable
fragments of first order logic.

The situation in ML is quite different concerning representation issues. ML aims at learning a model
of the world from data. There are thus two key representation problems: the representation of data and
the representation of models. See, e.g., [CKN20, CV20]. In many approaches the data space is assim-
ilated to a subset of R?, in which the observations are described by p numerical attributes. This is the
simplest case, allowing the use of mathematical results in linear algebra and in continuous optimization.
Nevertheless data may also be described by qualitative attributes, as for instance binary attributes, thus
requiring different mathematical approaches, based on discrete optimisation and on enumeration coupled
with efficient pruning strategies. Quite often, data is described by both types of attributes and only few
ML tools, for instance decision trees, are able to handle them. Therefore, changes of representation are
needed, as for instance discretization, or the encoding of qualitative attributes into numerical ones, all
inducing a bias on the learning process. More complex data, such as relational data, trees, graphs need
more powerful representation languages, such as first order logic or some proper representation trick as
for instance propositionalization or the definition of appropriate kernels. It is important to notice that the
more sophisticated the representation language, the more complex the inference process and a trade-off
must be found between the granularity of the representation and the efficiency of the ML tool.

Regarding models, they depend on the ML task: supervised or unsupervised classification, learning to
rank, mining frequent patterns, etc. They depend also on the type of approach that one favours: more
statistically or more artificial-intelligence oriented. There is thus a distinction between generative and dis-
criminative models (or decision functions). In the generative approach, one tries to learn a probability
distribution pg over the input space 2. If learning a precise enough probability distribution is successful,
it becomes possible in principle to generate further examples x € 2, the distribution of which is indis-
tinguishable from the true underlying distribution. It is sometimes claimed that this capability makes the
generative approach “explicative”. This is a matter of debate. The discriminative approach does not try to
learn a model that allows the generation of more examples. It only provides either a means of deciding
when in the supervised mode, or a means to express some regularities in the data set in the unsupervised
mode. These regularities as well as these decision functions can be expressed in terms of logical rules,
graphs, neural networks, etc. While they do not allow to generate new examples, they nonetheless can be
much more interpretable than probability distributions.

Very sketchily, one can distinguish between the following types of representations.

e Linear models and their generalisations, such as linear regression or the linear perceptron first pro-
posed by Rosenblatt [Ros58]. Because these models are based on linear weightings of the descriptors
of the entries, it looks easy to estimate the importance of each descriptor and thus to offer some under-
standing of the phenomenon at hand. This, however, assumes that the descriptors are uncorrelated
and are well chosen.

e Nonlinear models are often necessary in order to account for the intricacies of the world. Neural
networks, nowadays involving very numerous layers of non linearity, are presently the favourite
tools for representing and learning non linear models.

e Linear models as well as nonlinear ones provide a description of the world or of decision rules through
(finite) combinations of descriptors. They are parametric models. Another approach is to approxi-
mate the world by learning a non previously fixed number of prototypes and use a nearest-neighbour
technique to define decision functions. These systems are capable of handling any number of proto-
types as long as the can fit the data appropriately. Support Vector Machines (SVM) fall in this category
since they adjust the number of support vectors (learning examples) in order to fit the data. Here,
explaining a rule may mean providing a list of the most relevant prototypes that the rule uses.

e The above models are generally numerical in essence, and the associated learning mechanisms most
often rely on some optimisation process over the space of parameters. Another class of models relies
on logical descriptions, e.g., sets of clauses. Decision trees can also be considered as logic-based,
since each tree can be transformed into a set of clauses. The learning algorithms use more powerful
structures over the space of models than numerical models. In many cases the discrete nature of
the search space and the definition of a generality relation between formulas allow the organiza-
tion of models in a lattice and the design of heuristics to efficiently prune the search space. More
generally, these approaches are usually modeled as enumeration problems (e.g., pattern mining) or
discrete optimization problems (supervised learning, clustering, ...). Moreover such models offer
more opportunities to influence the learning process using prior knowledge. Finally, they can be
easily interpreted. The downside is their increased brittleness when coping with noisy data.

2.2 Computational Complexity

Complexity issues are a major concern in any branch of computer science. In KRR, very expressive repre-
sentation languages have been studied, but interesting reasoning problems for these languages are often
at least at the second level of the polynomial hierarchy for time complexity. There is a trade-off between
the expressive power of a language and the complexity of the inference it allows. Reasoning tasks in lan-
guages with suitably restricted expressivity are tractable, like for instance languages using Horn clauses or
Lightweight description logics such as DL-lite [CDL*05] or EL [BBLO5].

The study of complexity has motivated a large number of works in many fields of KRR including non-
monotonic reasoning, argumentation, belief merging and uncertainty management. In particular when
the desirable solution (i.e., gold standard) of the problem (for instance, merging operator, inconsistency-
tolerant consequence relation, etc.) has a high computational complexity, then it is common to look for an
approximation that has reasonable complexity. For instance, the observation that answering meaningful
queries from an inconsistent DL-Lite knowledge base using universal consequence relation is NP-Complete,
has led to the introduction of several tractable approximations [BBB* 16].

The attempt to cope with hardness of inference has also been a driving force in research around some
important and expressive languages, including propositional clauses and CSPs, where inference is NP-
complete; for instance, powerful methods nowadays enable the solving of SAT problems with up to hun-
dreds of thousands of variables, and millions of clauses in a few minutes (see section 4.8). Some of the
most competitive current SAT solvers are described in [AH15, MML14, LCW*15]. Two other ways to cope
with time complexity are anytime methods, which can be interrupted at any time during the solving process
and then return an incomplete, possibly false or sub-optimal solution; and approximate methods. A recent
trend in KRR is to study so-called compilation schemes [DM0O2, Mar15]: the idea here is to pre-process
some pieces of the available information in order to improve the computational efficiency (especially, the
time complexity) of some tasks; this pre-processing leads to a representation in a language where reason-
ing tasks can be performed in polynomial time (at the cost of a theoretical blow up in worst-case space
complexity, which fortunately does not often happen in practice).
Contrastingly, ML algorithms often have a time complexity which is polynomial in the number of vari-
ables, the size of the dataset and the size of the model being learnt, especially when the domains are
continuous. However, because of the possible huge size of the dataset or of the models, capping the de-
gree of the polynomial remains an important issue. In the case of discrete domains, finding the optimal
model,i.e., the one that best fits a given set of examples, can be hard (see [HR76] ), but one is often happy
with finding a “good enough” model in polynomial time: there is no absolute guarantee that the model that
best fits the examples is the target model anyway, since this may depend on the set of examples. In fact, an
important aspect of complexity in ML concerns the prediction of the quality of the model that one can learn
from a given dataset: in the PAC setting for instance [Val84], one tries to estimate how many examples are
needed to guarantee that the model learnt will be, with a high probability, a close approximation to the
unknown target model. Intuitively, the more expressive the hypothesis space is, the more difficult it will
be to correctly identify the target model, and the more examples will be needed for that [VC71].

2.3 Lack and Excess of Information: Uncertainty

With respect to a considered reasoning or decision task, information may be missing, or, on the contrary,
may be in excess, hence in conflict, which possibly generates uncertainty. Uncertainty has always been an
important topic in KRR [Hal17]. While in ML uncertainty is almost always considered to be of statistical
or probabilistic origin (aleatory uncertainty), other causes for uncertainty exist, such as the sheer lack of
knowledge, and the excess of information leading to conflicts (epistemic uncertainty). However, the role
of uncertainty handling in KRR and in ML seems to have been very different so far. While it has been an
important issue in KRR and has generated a lot of novel contributions beyond classical logic and probability,
it has been considered almost only from a purely statistical point of view in ML [Vap13].

The handling of uncertainty in KRR has a long history, as much with the handling of incomplete in-
formation in non-monotonic reasoning as with the handling of probabilities in Bayesian nets [Pea88], and
in probabilistic logic languages [Rus15, Coz20]. Other settings that focus on uncertainty due to incom-
plete information are possibility theory, with weighted logic bases (possibilistic logic [DLP94, DPS17]) and
graphical representations (possibilistic nets [BDGP0O2, BT12]). Belief functions also lend themselves to
graphical representations (valuation networks [She94], evidential networks [YM08]) and imprecise prob-
ability as well (credal nets [Coz05]).

Uncertainty theories distinct from standard probability theory, such as possibility theory or evidence
theory are now well-recognised in knowledge representation. They offer complementary views to un-
certainty with respect to probability, or as generalisations of it, dedicated to epistemic uncertainty when
information is imprecise or partly missing.

In KRR, at a more symbolic level, the inevitability of partial information has motivated the need for
exception-tolerant reasoning. For instance, one may provisionally conclude that “Tweety flies” while only
knowing that "Tweety is a bird”, although the default rule “birds fly" has exceptions, and we may later
conclude that “Tweety does not fly”, when getting more (factual) information about Tweety. Thus non-
monotonic reasoning [BMT11] has been developed for handling situations with incomplete data, where
only plausible tentative conclusions can be derived. Generic knowledge may be missing as well. For
example, one may not have the appropriate pieces of knowledge for concluding about some set of facts.
Then it may call for interpolation between rules [SP13a].

When information is in excess in KRR, it may mean that it is just redundant, but it becomes more likely
that some inconsistency appears. Redundancy is not always a burden, and may sometimes be an advantage
by making more things explicit in different formats (e.g., when looking for solutions to a set of constraints).

Inconsistency is a natural phenomenon in particular when trying to use information coming from dif-
ferent sources. Reasoning from inconsistent information is not possible in classical logic (without triviali-
sation). It has been extensively studied in AI [BH98, BDP97, CC16], in order to try and salvage non-trivial
conclusions not involved in contradictions. Inconsistency usually appears at the factual level, for instance
a logical base with no model. However, a set of rules may be said to be incoherent when there exists an
input fact that, together with the rules, would create inconsistency [AR90].

Machine Learning can face several types of situations regarding the amount of information available.
It must be said at once that induction, that goes from observations to regularities, is subject to the same
kind of conservation law as in Physics. The information extracted is not created, it is just a reformulation,
often with loss, of the incoming information.

If the input data is scarce, then prior knowledge, in one form or another, must complete it. The less data
is available, the more prior knowledge is needed to focus the search of regularities by the learning system.
This is in essence what the statistical theory of learning says [Vap13]. In recent years, lots of methods
have been developed to confront the case where data is scarce and the search space for regularities is
gigantic, specially when the number of descriptors is large, often in the thousands or more. The idea is
to express special constraints in the so-called regularization term in the inductive criterion that the system
use to search the hypothesis space. For instance, a constraint is often that the hypothesis should use a very
limited set of descriptors [Tib96].

When there is plenty of data, the problem is more one of dealing with potential inconsistencies. How-
ever, except in the symbolic machine learning methods, mostly studied in the 1980s, there is no systematic
or principled ways of dealing with inconsistent data. Either the data is pre-processed in order to remove
these inconsistencies, and this means having the appropriate prior knowledge to do so, or one relies on the
hope that the learning method is robust enough to these inconsistencies and can somehow smooth them
up. Too much data may also call for trying to identify a subset of representative data (a relevant sample), as
sometimes done in case-based reasoning, when removing redundant cases. Regarding the lack of data there
is a variety of approaches for the imputation of missing values ranging from the EM algorithm [DLR77 ] to
analogical proportion-based inference [BPR17]. However these methods get rid of incompleteness and do
not reason about uncertainty.

Finally, a situation that is increasingly encountered is that of multi-source data. Then, the characteristics
of the multiple data sets can vary, both in the format, the certainty, the precision, and so on. Techniques like
data fusion, data aggregation or data integration are called for, often resorting again to prior knowledge,
using for instance ontologies to enrich the data.

2.4 Causality and Explainability

"What is an explanation", "What has to be explained, and how" are issues that have been discussed for a
long time by psychologists and philosophers [Tha78, Bro92]. The interest in AI for explanations is not new
either. It appears with the development of rule-based expert systems in the mid-1980’s. Then there was a
natural need for explanations that are synthetic, informative, and understandable for the user of an expert
system [CDL19]. This raises issues such as designing strategic explanations for a diagnosis, for example in
order to try to lay bare the plans and methods used in reaching a goal [HCR84], or using “deep” knowledge
for improving explanations [Kas87 ]. Another issue was the ability to provide negative explanations (as well
as positive ones) for answering questions of the form “Why did you not conclude X?" [RS87], even in the
presence of uncertainty [FP89].

Introductory surveys about explanations in AI may be found in a series of recent papers [HK17a,
HMK17, Kle18, HMM*18, GBY+18, GTFA19]. Let us also mention the problem of explaining the results of
a multi-attribute preference model that is like a “black box”. It has been more recently studied in [Lab11].

As now discussed, explanations are often related to the idea of causality [HPO5]. Indeed most of the
explanations we produce or we expect involve some causal relationships (e.g., John imposed on himself
to go to the party because he thought that Mary would be there). In many domains where machines can
provide aid for decision making, as in medicine, court decisions, credit approval and so on, decision makers
and regulators more and more want to know what is the basis for the decision suggested by the machine,
why it should be made, and what alternative decision could have been made, had the situation been slightly
different. One of the difficulties of explainability in Machine learning is due to the fact that algorithms focus
on correlations between features and output variables rather than on causality. The example of wolf vs dog
identification [RSG16] perfectly illustrates this problem. When using a deep classifier, the main feature
that determines if a picture represents a dog or a wolf is the presence of snow. There obviously exists a
correlation between snow and wolves but it is clearly not a causality link. When this unwanted bias is
known, this can be corrected by adding constraint or balancing the dataset. However, due to the lack of
interpretability of some algorithms, identifying these biases is challenging. On the other hand, the problem
of constraining an algorithm to learn causality rather than correlation is still open.

If explainability is quickly becoming a hot topic in ML, while it was such for expert systems about 30
years ago, the search for solutions is still at an early stage. Some tentative distinction between interpretabil-
ity and explainability has been suggested.

Interpretability may have a statistical or a KR interpretation (which are of course not mutually exclu-
sive). From a statistical point of view, an interpretable model is a model that comes with mathematical
guarantees. They are usually bounds for the approximation errors (linked to the expression power of the
hypothesis space) or the generalization error (linked to the robustness of the algorithm with respect to
variations of the sample set). These can be also guarantees about the uncertainty around the parameters
of the model (represented by confidence intervals for instance). Linear approaches are, in this scope, the
most statistically interpretable ML algorithm. Robustness properties of statistical models are also desirable
for interpretable models. This is especially the case when considering explanations based on counterfactual
examples. Given a binary classifier and an example e, the counterfactual of e is the closest example to e
with respect to a metric that is labeled by the classifier with the opposite label of e (the counterfactual is
not necessarily in the dataset). Consider for instance a model that determines if a credit is allowed or not
with respect to the profile of a customer and a client to which the credit is not granted. The counterfactual
in this case answers the question of what is the minimal change on his profile that would ensure that the
credit is granted. If the model is based on propositional logic rules, the counterfactual will correspond to a
minimal change of the considered example representation in Boolean logic. In this case, the counterfactual
is an understandable explanation for the prediction. In deep-learning, the counterpart of counterfactuals
are adversarial examples [GSS14]. In most situations, an adversarial example corresponds to an imper-
ceptible modification of the considered example. From this point of view, the lack of robustness of deep
networks makes explanations based on counterfactuals very difficult to obtain.

A decision function learned by a system is assumed to be interpretable if it is simple, often meaning a
linear model with few parameters, or if it is expressed with terms or rules that a domain expert is supposed
to understand, like in rule-based systems or in decision trees. One influencing work promotes the use of
locally linear models in order to offer some interpretability even to globally non linear models [RSG16].

Explainability can be understood at two levels. Either at the level of the learning algorithm itself that
should be easily understandable by ML experts as well as by practitioners or users of the system. Or at
the level of the learned model that, for instance, could incorporate causal relationships. One question is:
is it possible to extract causal relationships from data alone, without some prior knowledge that suggest
those relationships? Judea Pearl [Pea09, PGJ16, PM18] argues that this is not possible, but gives to the
ML techniques the role of identifying possible correlations between variables in huge data sets that are
impossible to sift through for human experts. A recent work [LPNC*t 17] suggests that it would be possible
to identify the direction of a causal relationship from observational data. However, the necessary interplay
between ML, prior knowledge and reasoning is still a matter of debate.

When dealing with high-dimensional structured data (such as image or text), interpretable and explain-
able approaches (in a statistical or a KR point of view) are known to be less effective than heavy numerical
approaches such as bagging (random forest, gradient boosting) or deep learning [LBH15, GBC16]. Deep
learning models are neither explainable nor interpretable due to the large number of parameters and their
entanglement. There also exist few statistical results for deep learning and the currently known properties
are restricted to very specific architectures (see [ES15] for instance).

Some approaches have been proposed for improving the explainability of a deep learning algorithms
(or in some cases any black box algorithm). A first solution for explaining the solution is to analyze the
sensitivity of the prediction with respect to small variations of the input. For instance, activation maps
[ZKL*15] will emphasise the most important pixel of a picture for a given prediction. Although this type
of representation is easily readable, there are some cases where this is not enough for explanation. An-
other solution is to approximate the model (globally or locally) with an explainable one. Even when the
approximation error is reasonable, we have no guarantee that the interpretation associated with the sur-
rogate model is related to the way the initial model makes the prediction. In [LPNCt17], authors propose
to locally replace the model with a surrogate interpretable model. This allows to reduce the approxima-
tion error but it is based on a neighbourhood notion that can be difficult to define in high-dimensional
structured spaces. Moreover, using an interpretable/explainable model is not always a guarantee for ex-
plainable prediction. Indeed, a linear function with millions of parameters or a set of rules with thousand
of literals may be not readable at all.

3 Some Methodologies Combining Reasoning Principles and Learning

The idea of combining KRR ingredients with ML tools is not new. It has been done in different ways.
This section presents a series of examples of methodologies mixing KRR and ML ideas, with no intent
to be exhaustive however. Each subsection roughly follows the same structure, stating the goal of the
methodology, presenting its main aspects, and identifying the KR and the ML parts.

3.1 Injecting Knowledge in Learning

Induction cannot be made to work without prior knowledge that restrains the space of models to be ex-
plored. Two forms of prior knowledge, aka. biases, are distinguished: representation biases that limit the
expressiveness of the language used to express the possible hypotheses on the world, and search biases that
control how the hypothesis space is explored by the learning algorithm.

Representation biases can take various forms. They can directly affect the language in which the possible
hypotheses can be expressed. For instance, “hypotheses can involve a maximum of two disjuncts”. In the
same way, but less declaratively, looking for linear models only is a severe representation bias. Often,
one does not want to be so strict, and prefers to favour more flexible preference criteria over the space of
possible hypotheses. Generally, this is expressed through a regularized optimisation criterion that balances
a measure of fit of the model to the data, and a measure of fit of the model to the bias. For instance, the
following quality measure over linear hypotheses h(x) = Bot Di §;x;,; for regression favours hypotheses
that involve fewer parameters:

1 m Pp 2 p
Rh) = 54] Yi BoD Bix; | + a> Billo
i=1 j=l j=l
fit to the data Favors models with few non zero parameters

where the Lo norm ||.||) counts the nonzero parameters f;.

The search bias dictates how the learning algorithm explores the space of hypotheses. For instance, in
the case of neural networks, the search starts with a randomly initialized neural network and then proceeds
by a gradient descent optimization scheme. In some other learning methods, such as learning with version
space, the search uses generalization relations between hypotheses in order to converge towards good
hypotheses. In this latter case, it is more easy to incorporate prior knowledge from the experts. Indeed,
the exploration of the hypothesis space is akin to a reasoning process, very much like theorem proving.

3.2 Inductive Logic Programming

Inductive Logic Programming (ILP) (see [MD94, De 08, DLO1] for general presentations) is a subfield of
ML that aims at learning models expressed in (subsets of) First Order Logic. It is an illustration of Symbolic
Learning, where the hypotheses space is discrete and structured by a generality relation. The aim is then
to find a hypothesis that covers the positive examples (it is then said to be complete) and rejects the
negative ones (it is said to be consistent). The structure of the hypothesis space allows to generalize an
incomplete hypothesis, so as to cover more positive examples, or to specialize an inconsistent hypothesis
in order to exclude negative covered examples. The main reasoning mechanism is induction in the sense
of generalization (subsumption).

In ILP examples and models are represented by clauses. Relying on First Order Logic allows to model
complex problems, involving structured objects (for instance to determine whether a molecule is active or
not, a system must take into account the fact it is composed of atoms with their own properties and shared
relations), or involving objects in relation with each other (a social network or temporal data). Reasoning
is a key part of ILP First, the search for a model is usually performed by exploring a search space structured
by a generality relation. A key point is then the definition of a generality relation between clauses. The
more natural definition of subsumption should be expressed in terms of logical consequences, which allows
comparing the models of both formula, but since the problem is in general not decidable, the notion of
@-subsumption, as introduced in [Plo70] is usually preferred: a clause C, is more general that a clause C,
if there exists a substitution 0 such that C,.@ € Cy. In this definition a clause, i.e. a disjunction of literals,
is represented by its set of literals. For instance, the rule par(X,Y),par(Y,Z) — grand_par(X,Z) 0-
subsumes par(john, ann), par(ann, peter), par(john,luc) — grand_par(john, peter ).Indeed, the first
one leads to the clause spar(X, Y)V-par(Y,Z)Vgrand_par(X, Z) and the second one =par(john, ann)V
—par(ann, peter) V ~par(john, luc) V grand_par(john, peter). Second, expert knowledge can be ex-
pressed using facts (ground atoms) or by rules, or yet reasoning mechanisms to be applied. This can be
illustrated by the well-known systems FOIL [Qui96] and Progol [Mug95]

ILP and more generally Symbolic Learning, has thus some interesting properties. First, the model
is expressed in logic and therefore is claimed to be easily understandable by a user (See for instance
[MSZ*18] for an interesting study of the comprehensibility or not of programs learned with ILP). Second,
expert knowledge can be easily expressed by means of clauses and integrated into the learning algorithm.
Although initially developed for the induction of logic programs, it has now shown its interest for learning
with structured data.

However, ILP suffers from two drawbacks: the complexity of its algorithms and its inability to deal
with uncertain data. Several mechanisms have been introduced to reduce the complexity, as for instance
the introduction of syntactic biases, restricting the class of clauses that can be learned. Another interesting
idea is propositionnalization, introduced in [LDG91] and then developed for instance in the system RSD
[ZLO6]. It is a process that transforms a relational problem into a classical attribute-value problem by the
introduction of new features capturing relations between objects. Once the transformation performed, any
supervised learner can be applied to the problem. The main difficulty is then to define these new features.

This last problem has led to the emergence of Statistical Relational Learning [GT07, DFKM08] that aims
at coupling ILP with probabilistic models. Many systems have been developed, extending naive Bayesian
classifier [LFO2], Bayesian Networks [FBBRO5] or Markov Logic Networks [RD06] or developing new prob-
abilistic framework as in Problog [DKTO7 ]. In all these works, inference and learning are tightly connected
since learning parameters requires to maximize the likelihood for generative learning (estimation of the
probabilities to generate the data, given a set of parameters), or the conditional likelihood in case of
discriminative learning (estimation of the probabilities of the labels given the data). Optimizing the pa-
rameters thus require at each step to estimate the corresponding probabilities. This has led to intensive
research on the complexity of inference.

In the last decade new works have emerged linking deep learning and Inductive Logic Programming.
Two directions are investigated. The first one relies on propositionnalization as in [KKK* 17]: first, a set of
interpretable rules is built through a Path Ranking Algorithm and then the examples are transformed into
an attribute-value representation. Two settings are considered: an existential one making the feature true
if an instantiation of this path exists in the example, a counting one that counts the number of times the
random walk is satisfied. Once this transformation is performed, a multilayered discriminative RBM can be
applied. The second direction, illustrated in [SSZ*+17], consists in encoding directly facts and ground rules
as neurons. Aggregation neurons allow combining rules with the same head and the activation function
are approximations of Lukasiewicz fuzzy logic.

3.3 Neuro-Symbolic Reasoning

Several works have proposed to combine learning and reasoning by studying schemes to translate logical
representations of knowledge into neural networks. A long-term goal of a series of works on neural-
symbolic integration, surveyed for instance by [BdGB* 17], is “to provide a coherent, unifying view for logic
and connectionism ... fin order to] ... produce better computational tools for integrated ML and reasoning."
Typical works propose translation algorithms from a symbolic to a connectionist representation, enabling
the use of computation methods associated with neural networks to perform tasks associated with the
symbolic representation.

Early works in this vein are [Pin91, Pin95, BHKS98, HKS99]. Bornscheuer et al. [BHKS98] show
for example how an instance of the Boolean satisfiability problem can be translated into a feed-forward
network that parallelizes GSAT, a local-search algorithm for Boolean satisfiability. They also show that a
normal logic program P can be turned into a connectionnist network that can approximate arbitrarily well
the semantics of the program.

The paper [dGZ99] exploits this idea to represent propositional logic programs with recurrent neural
networks (RNNs) which can be used to compute the semantics of the program. They show that this program
can also be used as background knowledge to learn from examples, using back-propagation. Essentially,
the RNN defined to represent a logic program P has all atoms of P in the input layer; one neuron, a kind of
“and” gate, for each rule in a single hidden layer; and one neuron for every atom in the output layer, these
neurons working like “or” gates. Re-entrant connections from an atom in the output layer to its counterpart
in the input layer enable the chaining of rules.

Franca et al. [FZdG14] extend these results to first-order programs, using a propositionalization method
called Bottom Clause Propositionalization. In [dGLGO7, dGLO3], methods are proposed to translate for-
mulas that contain modalities into neural networks, enabling the representation of time and knowledge,
and the authors in [dGLG03, dGLG06] show that there exists a neural network ensemble that computes
a fixed-point semantics of an intuitionistic theory. Pinkas and Cohen [PC19] perform experiments with
so-called higher-order sigma-pi units (which compute a sum of products of their inputs) instead of hidden
layers, for planning problems on simple block-world problems: the number of units is fixed at the design
time, and is a function of the maximum number of blocks and the maximum number of time steps; for
example, for every pair (b,, bz) of possible blocks and every time step t, there is a unit representing the
proposition above(b,, b2,t); their results indicate that a learning phase enables the network to approxi-
mately learn the constraints with a reasonable amount of iterations, which speeds up the computation of
an approximate solution for subsequent instances.

A new direction of research has recently emerged, bringing in particular to Statistical Relational Learn-
ing [GT07, DKNP16] the power of tensor networks. For instance, in [YYC17, HL17], they use recursive
tensor networks to predict classes and / or binary relations from a given knowledge base. It has also
been proposed in, e.g., [DGMR12, SCMN13, SdG16] to depart from the usual semantics of logic based
on Boolean truth values in order to use the numerical operators of neural networks. The universe of an
interpretation of first-order logic can be a set of vectors of real numbers, and the truth values of predi-
cates can be real numbers in the interval [0, 1]; truth values of general formulas can then be defined using
usual operators of fuzzy logic. Donadello et al. [DSdG17] describe how this approach can be used to learn
semantic image interpretation using background knowledge in the form of simple first-order formulas.

3.4 Formal Concept Analysis

Formal Concept Analysis (FCA) [GW98, FKHt 20] is another example of a setting that stands in between
KRR and ML concerns. It offers a mathematical framework that is based on the duality between a set of
objects or items and a set of descriptors. In the basic setting, we start from a formal context which is a
relation linking objects and (Boolean) attributes (or properties). Thus a formal context constitutes a simple
repository of data. A concept is formalized as a pair composed of a set of attributes and a set of objects
representing the intention and the extension of the concept respectively; it has the property that these
objects and only them satisfy the set of attributes and this set of attributes refers to these objects and only
them. Such a set of attributes is called a closed pattern or a closed itemset. More precisely, two operators,
forming a Galois connection, respectively associate their common descriptors to a subset of objects, and
the set of objects that satisfy all of them to a subset of descriptors. In an equivalent way, a pair made of
a set of objects and a set of attributes is a formal concept if and only if their Cartesian product forms a
maximal rectangle for set inclusion in the formal context. The set of concepts forms a complete lattice.

Formal Concept Analysis is deeply rooted in Artificial Intelligence by the formalization of the notion of
concept. Recent years have witnessed a renewed interest in FCA with the emergence of Pattern Mining:
it has been shown that the set of closed itemsets forms a condensed representation of the set of itemsets,
thus reducing the memory space for storing them. Moreover it a possible to define an equivalence relation
between itemsets (two itemsets are equivalent if they share the same closure) and from the equivalence
classes, it becomes possible to extract all the exact association rules (rules with a confidence equal to 1).
See [GD86, PBTL99, BPTT00]

Two extensions are especially worth mentioning. One uses fuzzy contexts, where the links between
objects and attributes are a matter of degree [Bel02]. This may be useful for handling numerical attributes
[MDNS08]. Another extension allows for structured or logical descriptors using so-called pattern structures

10
[GKO1, FRO4, AKP10]. Besides, operators other than the ones defining formal concepts make sense in
formal concept analysis, for instance to characterize independent subcontexts [DP12].

3.5 Rule-Based Models

Knowledge representation by if-then rules is a format whose importance was early acknowledged in the
history of AI, with the advent of rule-based expert systems. Their modelling has raised the question of the
adequacy of classical logic for representing them, especially in case of uncertainty where conditioning is
often preferred to material implication. Moreover, the need for rules tolerating exceptions, or expressing
gradedness, such as default rules and fuzzy rules has led KRR to develop tools beyond classical logic.

3.5.1 Default rules

Reasoning in a proper way with default rules (i.e., having potential exceptions) was a challenging task for
AI during three decades [BMT11]. Then a natural question is: can rules having exceptions extracted from
data be processed by a nonmonotonic inference system yielding new default rules? How can we insure
that these new rules are still agreeing with the data? The problem is then to extract genuine default rules
that hold in a Boolean database. It does not just amount to mining association rules with a sufficiently high
confidence level. We have to guarantee that any new default rule that is deducible from the set of extracted
default rules is indeed valid with respect to the database. To this end, we need a probabilistic semantics
for nonmonotonic inference. It has been shown [BDP99] that default rules of the form “if p then generally
q”, denoted by p ~» q, where ~» obey the postulates of preferential inference [KLM90], have both

1. a possibilistic semantics expressed by the constraint II(p Aq) > II(pA-q), for any max-decomposable
possibility measure II (II(p V qg) = max(II(p), II(q))),

2. a probabilistic semantics expressed by the constraint Prob(p Aq) > Prob(p A7q) for any big-stepped
probability Prob.

This is a very special kind of probability such that if p; > py >... > Dp_1 = Pn (where p; is the probability
of one of the n possible worlds), the following inequalities hold Vi = 1,n—1, pj > Yjajn pj. Then,
one can safely infer a new default p ~» q from a set of defaults A = {p, ~ q,|k = 1,K} if and only if
the constraints modeling A entail the constraints modeling p ~» q. Thus, extracting defaults amounts to
looking for big-stepped probabilities, by clustering lines describing items in Boolean tables, so as to find
default rules, see [BDLP03] for details. Then the rules discovered are genuine default rules that can be
reused in a nonmonotonic inference system, and can be encoded in possibilistic logic (assuming rational
monotony for the inference relation).

It may be also beneficial to rank-order a set of rules expressed in the setting of classical logic in order
to handle exceptions in agreement with nonmonotonic reasoning. This what has been proposed in [SP07]
where a new formalization of inductive logic programming (ILP) in first-order possibilistic logic allows us
to handle exceptions by means of prioritized rules. Indeed, in classical first-order logic, exceptions of the
rules can be assigned to more than one class, even if only one is the right one, which is not correct. The
possibilistic formalization provides a sound encoding of non-monotonic reasoning that copes with rules
with exceptions and prevents an example from being classified in more than one class.

Possibilistic logic [DLP94] is also a basic logic for handling epistemic uncertainty. It has been established
that any set of Markov logic formulas [RDO6] can be exactly translated into possibilistic logic formulas
[KDS15, DPS17], thus providing an interesting bridge between KRR and ML concerns.

3.5.2 Fuzzy rules

The idea of fuzzy if-then rules was first proposed by L. A. Zadeh [Zad73]. They are rules whose conditions
and /or conclusions express fuzzy restrictions on the possible values of variables. Reasoning with fuzzy
rules is based on a combination / projection mechanism [Zad79] where the fuzzy pieces of information
(rules, facts) are conjunctively combined and projected on variables of interest. Special views of fuzzy rules
have been used when designing fuzzy rule-based controllers: fuzzy rules may specify the fuzzy graph of a

11
control law which once applied to an input yields a fuzzy output that is usually defuzzified [MA75]. Or rules
may have precise conclusions that are combined on the basis of the degrees of matching between the current
situation and the fuzzy condition parts of the rules [TS85]. In both cases, an interpolation mechanism is at
work, implicitly or explicitly [Zad92]. Fuzzy rules-based controllers are universal approximators [Cas95].
The functional equivalence between a radial basis function-based neural network and a fuzzy inference
system has been established under certain conditions [JS93].

Moreover, fuzzy rules may provide a rule-based interpretation [dAN94, SHHO2] for (simple) neural
nets, and neural networks can be used for extracting fuzzy rules from the training data [Ped98, GLO5].
Regarding neural nets, let us also mention a non-monotonic inference view [BG91, Gar91].

Association rules [AIS93, HH78] describe relations between variables together with confidence and
support degrees. See [DHP06] for the proper assessment of confidence and support degrees in the fuzzy
case. In the same spirit, learning methods for fuzzy decision trees have been devised in [MB10], in the
case of numerical attributes. The use of fuzzy sets to describe associations between data and decision
trees may have some interest: extending the types of relations that may be represented, making easier the
interpretation of rules in linguistic terms [DPS05], and avoiding unnatural boundaries in the partitioning
of the attribute domains.

There are other kinds of fuzzy rules whose primary goal is not to approximate functions nor to quantify
associations, but rather to offer representation formats of interest. This is, for instance, the case of gradual
rules, which express statements of the form “the more x is A, the more y is B, where A, and B are gradual
properties modelled by fuzzy sets [SDPSO7, NLP10].

3.5.3 Threshold rules

Another format of interest is the one of multiple threshold rules, i.e., selection rules of the form “if x; > a,
and --- x; >a, and--- then y = y” (or deletion rules of the form ‘if x, < 6, and --- x; < 8; and--- then
y = 5”), which are useful in monotone classification / regression problems [GISO6, BSS11]. Indeed when
dealing with data that are made of a collection of pairs (xk, yz), k = 1,...,N, where x*isa tuple (xk, wey x*)
of feature evaluations of item k, and where y is assumed to increase with the x;’s in the broad sense, it is
of interest of describing the data with such rules of various lengths. It has been noticed [GMS04, DPR14]
that, once the numerical data are normalized between 0 and 1, rules where all (non trivial) thresholds are
equal can be represented by Sugeno integrals (a generalization of weighted min and weighted max, which
is a qualitative counterpart of Choquet integrals [GL10]). Moreover, it has been shown recently [BCD*t 18]
that generalized forms of Sugeno integrals are able to describe a global (increasing) function, taking values
on a finite linearly ordered scale, under the form of general thresholded rules. Another approach, in the
spirit of the version space approach [Mit79], provides a bracketing of an increasing function by means of
a pair of Sugeno integrals [PRSRO9, PRSO9].

3.6 Uncertainty in ML: in the data or in the model

We will focus on two aspects in which cross-fertilisation of ML with KRR could be envisaged: uncertainty
in the data and uncertainty in the models/predictions.

3.6.1 Learning under uncertain and coarse data

In general, learning methods assume the data to be complete, typically in the form of examples being
precise values (in the unsupervised case) or precise input/output pairs (in the supervised case). There are
however various situations where data can be expected to be uncertain, such as when they are provided
by human annotators in classification or measured by low-quality sensors, or even missing, such as when
sensors have failed or when only a few examples could be labelled. An important remark is that the
uncertainty attached to a particular piece of data can hardly be said to be of objective nature (representing
frequency) as it has a unique value, and this even if this uncertainty is due to an aleatory process.

While the case of missing (fully imprecise) data is rather well-explored in the statistical [LR19] and
learning [CSZ06] literature, the general case of uncertain data, where this uncertainty can be modelled

12
using different representation tools of the literature, largely remains to be explored. In general, we can
distinguish between two strategies:

e The first one intends to extend the precise methods so that they can handle uncertain data, still
retrieving a precise model from them. The most notable approaches consist in either extending
the likelihood principle to uncertain data (e.g., [Den13] for evidential data, or [CD18] for coarse
data), or to provide a precise loss function defined over partial data and then using it to estimate
the empirical risk, see for instance [Hiil114, CST11, CS12]. Such approaches are sometimes based on
specific assumptions, usually hard to check, about the process that makes data uncertain or partial.
Some other approaches such as the evidential likelihood approach outlined in [Den13] do not start
from such assumptions, and simply propose a generic way to deal with uncertain data. We can
also mention transductive methods such as the evidential K-nearest neighbour (K-NN) rule [Den95,
DZ01, DKS19], which allows one to handle partial (or “soft”) class labels without having to learn a
model.

e The second approach, much less explored, intends to make no assumptions at all about the underlying
process making the data uncertain, and considers building the set of all possible models consistent
with the data. Again, we can find proposals that extend probability-based approaches [DCZ04], as
well as loss-based ones [CS16]. The main criticism one could address to such approaches is that they
are computationally very challenging. Moreover they do not yield a single predictive model, making
the prediction step potentially difficult and ill-defined, but also more robust.

The problem of handling partial and uncertain data is certainly widely recognised in the different fields
of artificial intelligence, be it KRR or ML. One remark is that mainstream ML has, so far, almost exclu-
sively focused on providing computationally efficient learning procedures adapted to imprecise data given
in the form of sets, as well as the associated assumptions under which such a learning procedure may
work [LD14]. While there are proposals around that envisage the handling of more complex form of un-
certain data than just sets, such approaches remain marginal, at least for two major reasons:

e More complex uncertainty models require more efforts at the data collection step, and the benefits
of such an approach (compared to set-valued data or noisy precise data) do not always justify the
additional efforts. However, there are applications in which the modeling of data uncertainty in the
belief function framework does improve the performances in classification tasks [COC*12, QDL17].
Another possibility could be that those data are themselves prediction of an uncertain model, then
used in further learning procedures (such as in stacking [DZ04]);

e Using more complex representations may involve a higher computational cost, and the potential gain
of using such representations is not always worth the try. However, some specific cases approaches
such as the EM algorithm [Den13] or the K-NN rule [DKS19] in the evidential setting, make it possible
to handle uncertain data without additional cost.

3.6.2 Uncertainty in the prediction model

Another step of the learning process where uncertainty can play an important role is in the characterisa-
tion of the model or its output values. In the following, we will limit ourselves to the supervised setting
where we search to learn a (predictive) function f : 2 — Y linking an input observation x € & to an
output (prediction) y € %. Assessing the confidence one has in a prediction can be important in sensitive
applications. This can be done in different ways:

e By directly impacting the model f itself, for instance associating to every instance x not a determin-
istic prediction f(x), but an uncertainty model over the domain %. The most common one is of
course probabilities, but other solutions such as possibility distributions, belief functions or convex
sets of probabilities are possible;

e By allowing the prediction to become imprecise, the main idea behind such a strategy being to have
weaker yet more reliable predictions. In the classical setting, this is usually done by an adequate
replacement of the loss function [Ha97, GRKC09], yet recent approaches take a different road. For

13
instance, imprecise probabilistic approaches consider sets of models combined with a skeptic in-
ference (also a typical approach in KR), where a prediction is rejected if it is so for every possible
model [CAZ12]. Conformal prediction [SVO8] is another approach that can be plugged to any model
output to obtain set-valued predictions. Approaches to quantify statistical predictions in the belief
function framework are described in [KSD16, XDZD16, DL18].

If such approaches are relatively well characterised for the simpler cases of multi-class classification, ex-
tending them to more complex settings such as multi-label or ranking learning problems that involve com-
binatorial spaces remain largely unexplored, with only a few contributions [CHWW12, AC17]. It is quite
possible that classical AI tools such as SAT or CSP solvers could help to deal with such combinatorial spaces.

3.7 Case-Based Reasoning, Analogical Reasoning and Transfer Learning

Case-based reasoning (CBR for short), e.g., [AP94] is a form of reasoning that exploits data (rather than
knowledge) under the form of cases, often viewed as pairs (problem, solution). When one seeks for poten-
tial solution(s) to a new problem, one looks for previous solutions to similar problems in the repertory of
cases, and then adapts them (if necessary) to the new problem.

Case-based reasoning, especially when similarity is a matter of degree, thus appears to be close to k-
NN methods and instance-based learning [Den95, HDP02]. The k-NN method is a prototypical example
of transduction, i.e., the class of a new piece of data is predicted on the basis of previously observed data,
without any attempt at inducing a generic model for the observed data. The term transduction was coined
in [GVV98], but the idea dates back to Bertrand Russell [Rus12].

Another example of transduction is analogical proportion-based learning. Analogical proportions are
statements of the form “a is to b as c is to d”, often denoted by a: b:: c: d, which express that “a differs
from b as c differs from d and b differs from a as d differs from c”. This statement can be encoded into
a Boolean logical expression [MP09, PR13] which is true only for the 6 following assignments (0,0, 0,0),
(1,1,1,1), (1, 0, 1, 0), (0, 1,0, 1), (1,1, 0,0), and (0,0, 1,1) for (a, b,c, d). Note that they are also compati-
ble with the arithmetic proportion definition a— b = c—d, where a— b € {—1,0, 1}, which is not a Boolean
expression. Boolean Analogical proportions straightforwardly extend to vectors of attributes values such
as @ = (aj,...,d,), by stating @: b::@: d iff Vie[1,n], a; : b; :: c; : d;. The basic analogical inference
pattern [SYO5], is then

Vie {l,...,p}, a; : bj ::¢;: d; holds
Vje{pt+1,..,n}, aj: bj ::¢;:d; holds

Thus analogical reasoning amounts to finding completely informed triples (a, b,2) appropriate for in-
ferring the missing value(s) in d. When there exist several suitable triples, possibly leading to distinct
conclusions, one may use a majority vote for concluding. This inference method extends to analogical
proportions between numerical values, and the analogical proportion becomes graded [DPR16]. It has
been successfully applied, for Boolean, nominal or numerical attributes, to classification [MBD08, BPR17]
(then the class cl(X) (viewed as a nominal attribute) is the unique solution, when it exists, such as
cl(@): cl(b) :: cl(€) : cl(¥) holds), and more recently to case-based reasoning [LNPR18] and to preference
learning [FH18, BPP18]. It has been theoretically established that analogical classifiers always yield ex-
act prediction for Boolean affine functions (which includes x-or functions), and only for them [CHPR17].
Good results can still be obtained in other cases [CHPR18]. Moreover, analogical inequalities [PR18b] of
the form “a is to b at least as much as c is to d” might be useful for describing relations between features
in images, as in [LTC17].

The idea of transfer learning, which may be viewed as a kind of analogical reasoning performed at
the meta level, is to take advantage of what has been learnt on a source domain in order to improve the
learning process in a target domain related to the source domain. When studying a new problem or a new
domain, it is natural to try to identify a related, better mastered, problem or domain from which, hopefully,
some useful information can be called upon for help. The emerging area of transfer learning is concerned
with finding methods to transfer useful knowledge from a known source domain to a less known target
domain.

The easiest and most studied problem is encountered in supervised learning. There, it is supposed that
a decision function has been learned in the source domain and that a limited amount of training data is

14
available in the target domain. For instance, suppose we have learnt a decision function that is able to
recognize poppy fields in satellite images. Then the question is: could we use this in order to learn to
recognize cancerous cells in biopsies rather than to start anew on this problem or when few labeled data
is available in the biological domain?

This type of transfer problem has witnessed a spectacular rise of interest in recent years thanks both
to the big data area that makes lots of data available in some domains, and to the onset of deep neural
networks. In deep neural networks, the first layers of neuron like elements elaborate on the raw input
descriptions by selecting relevant descriptors, while the last layers learn a decision function using these
descriptors. Nowadays, most transfer learning methods rely on the idea of transferring the first layers
when learning a new neural network on the target training data, adjusting only the last layers. The un-
derlying motivation is that the descriptors are useful in both the source and target domains and what is
specific is the decision function built upon these descriptors. But it could be defended just as well that the
decision function is what is essential in both domains while the ML part should concentrate on learning an
appropriate representation. This has been achieved with success in various tasks [CAMO17].

One central question is how to control what should be transferred. A common assumption is that
transfer learning should involve a minimal amount of change of the source domain knowledge in order
for it to be used in the target domain. Several ways of measuring this “amount of change” have been
put forward (see for instance [CFTR16, SQZY17]), but much work remains to be done before a satisfying
theory is obtained.

One interesting line of work is related to the study of causality. Judea Pearl uses the term “transporta-
bility” instead of transfer learning, but the fundamental issues are the same. Together with colleagues, they
have proposed ways of knowing if and what could be transferred from one domain to another [PM18]. The
principles rely on descriptions of the domains using causal diagrams. Thanks to the “do-calculus”, formal
rules can be used in order to identify what can be used from a source domain to help solve questions in
the target domain. One foremost assumption is that causality relationships capture deep knowledge about
domains and are somewhat preserved between different situations. For instance, proverbs in natural lan-
guage are a way of encapsulating such deep causality relationships and their attractiveness comes from
their usefulness in many domains or situations, when properly translated.

4 Examples of KRR/ML Synergies

In the previous section, we have surveyed various paradigms where KRR and ML aspects are intricately
entwined together. In this section, we rather review examples of hybridizations where KRR and ML tools
cooperate. In each case, we try to identify the purpose, the way the KRR and ML parts interact, and the
expected benefits of this synergy.

4.1 Dempster-Shafer Reasoning and Generalized Logistic Regression Classifiers

The theory of belief functions originates from Dempster’s seminal work [Dem67] who proposed, at the end
of the 1960’s, a method of statistical inference that extends both Fisher’s fiducial inference and Bayesian
inference. In a landmark book, Shafer [Sha76] developed Dempster’s mathematical framework and ex-
tended its domain of application by showing that it could be proposed as a general language to express
“probability judgements” (or degrees of belief) induced by items of evidence. This new theory rapidly
became popular in Artificial Intelligence where it was named “Dempster-Shafer (DS) theory”, evidence
theory, or the theory of belief functions. DS theory can be considered from different perspectives:

e A belief function can be defined axiomatically as a Choquet monotone capacity of infinite order
[Sha76].

e Belief functions are intimately related to the theory of random sets: any random set induces a belief
function and, conversely, any belief function can be seen as being induced by a random set [Ngu78].

e Sets are in one-to-one correspondence with so-called “logical” belief functions, and probability mea-
sures are special belief functions. A belief function can thus be seen both as a generalised proba-
bility measure and as a generalised set; it makes it possible to combine reasoning mechanisms from

15
probability theory (conditioning, marginalisation), with set-theoretic operations (intersection, union,
cylindrical extension, interval computations, etc.)

DS theory thus provides a very general framework allowing us to reason with imprecise and uncertain
information. In particular, it makes it possible to represent states of knowledge close to total ignorance and,
consequently, to model situations in which the available knowledge is too limited to be properly represented
in the probabilistic formalism. Dempster’s rule of combination [Sha76] is an important building block of
DS theory, in that it provides a general mechanism for combining independent pieces of evidence.

The first applications of DS theory to machine learning date back to the 1990’s and concerned classifier
combination [XKS92, Rog94], each classifier being considered as a piece of evidence and combined by
Dempster’s rule (see, e.g., [QMD11] for a refinement of this idea taking into account the dependence
between classifier outputs). In [Den95], Denceux combined Shafer’s idea of evidence combination with
distance-based classification to introduce the evidential K-NN classifier [Den95]. In this method, each
neighbour of an instance to be classified is considered as a piece of evidence about the class of that instance
and is represented by a belief function. The K belief functions induced by the K nearest neighbour are then
combined by Dempster’s rule. Extensions of this simple scheme were later introduced in [ZD98, LRD16,
DKS19].

The evidential K-NN rule is, thus, the first example of an “evidential classifier”. Typically, an eviden-
tial classifier breaks down the evidence of each input feature vector into elementary mass functions and
combines them by Dempster’s rule. The combined mass function can then be used for decision-making.
Thanks to the generality and expressiveness of the belief function formalism, evidential classifiers provide
more informative outputs than those of conventional classifiers. This expressiveness can be exploited, in
particular, for uncertainty quantification, novelty detection and information fusion in decision-aid or fully
automatic decision systems [DKS19].

In [DKS19], it is shown that that not only distance-based classifiers such as the evidential K-NN rule,
but also a broad class of supervised machine learning algorithms, can be seen as evidential classifiers.
This class contains logistic regression and its non linear generalizations, including multilayer feedforward
neural networks, generalized additive models, support vector machines and, more generally, all classifiers
based on linear combinations of input or higher-order features and their transformation through the lo-
gistic or softmax transfer function. Such generalized logistic regression classifiers can be seen as combining
elementary pieces of evidence supporting each class or its complement using Dempster’s rule. The output
class probabilities are then normalized plausibilities according to some underlying belief function. This
“hidden” belief function provides a more informative description of the classifier output than the class
probabilities, and can be used for decision-making. Also, the individual belief functions computed by each
of the features provide insight into the internal operation of classifier and can help to interpret its decisions.
This finding opens a new perspective for the study and practical application of a wide range of machine
learning algorithms.

4.2. Maximum Likelihood Under Coarse Data

When data is missing or just imprecise (one then speaks of coarse data), statistical methods need to be
adapted. In particular the question is whether one wishes to model the observed phenomenon along with
the limited precision of observations, or despite imprecision. The latter view comes down to complete the
data in some way (using imputation methods). A well-known method that does it is the EM algorithm
[DLR77 ]. This technique makes strong assumptions on the measurement process so as to relate the distri-
bution ruling the underlying phenomenon and the one ruling the imprecise outcomes.It possesses variants
based on belief functions [Den13]. EM is extensively used for clustering (using Gaussian mixtures) and
learning Bayesian nets.

However the obtained result, where by virtue of the algorithm, data has become complete and precise,
is not easy to interpret. If we want to be faithful to the data and its imperfections, one way is to build a
model that accounts for the imprecision of observations, i.e., a set-valued model. This is the case if a belief
function is obtained via maximum likelihood on imprecise observations: one optimises the visible likelihood
function [CD18]. The idea is to cover all precise models that could have been derived, had the data been
precise. Imprecise models are useful to lay bare ignorance when it is present, so as to urge finding more

16
data, but it may be problematic for decision or prediction problems, when we have to act or select a value
despite ignorance.

Ideally we should optimize the likelihood function based on the actual values hidden behind the im-
precise observations. But such a likelihood function is ill-known in the case of coarse data [CD18]. In that
case, we are bound

e To make assumptions on the measurement process so as to create a tight link between the hidden
likelihood function pertaining to the outcomes of the real phenomenon, and the visible likelihood of
the imprecise observations (for instance the CAR (coarsening at random) assumption [HR91], or the
superset assumption [HC15]. In that case, the coarseness of the data can be in some sense ignored.
See [Jae05] for a general discussion.

e Or to pick a suitable hidden likelihood function among the ones compatible with the imprecise data,
for instance using an optimistic maximax approach that considers that the true sample is the best
possible sample in terms of likelihood compatible with the imprecise observation [Hiil14]. This
approach chooses a compatible probability distribution with the minimum of entropy, hence tends
to disambiguate the data. On the contrary maximin approach considers that the true sample is the
worst compatible sample in terms of likelihood. This approach chooses a compatible probability
distribution with the maximum of entropy. Those two approaches suppose extreme point of view on
the entropy of the probability distribution. More recently, an approach base on the likelihood ratio
that maximize the minimal possible ratio over the compatible probability distribution is proposed
in [GD18]. This approach achieving a trade-off between these two more extreme approaches and is
able to quantify the quality of the chosen probability distribution in regards to all possible probability
distribution. In these approaches, the measurement process is ignored.

See [CDH17, HDC19] for more discussions about such methods for statistical inference with poor quality
data.

Besides, another line of work for taking into account the scarcity of data in ML is to use anew cumulative
entropy-like function that together considers the entropy of the probability distribution and the uncertainty
pertaining to the estimation of its parameters. It takes advantage of the ability of a possibility distribution to
upper bound a family of probabilities previously estimated from a limited set of examples [SP13b, SP15].
Such a function takes advantage of the ability of a possibility distribution to upper bound a family of
probabilities previously estimated from a limited set of examples and of the link between possibilistic
specificity order and entropy [DH07]. This approach enables the expansion of decision trees to be limited
when the number of examples at the current final nodes is too small.

4.3 EM Algorithm and Revision

Injecting concepts from KRR, when explaining the EM algorithm may help better figure out what it does.
In the most usual case, the coarse data are elements of a partition of the domain of values of some hidden
variable. Given a class of parametric statistical models, the idea is to iteratively construct a precise model
that fits the data as much as possible, by first generating at each step a precise observation sample in
agreement with the incomplete data, followed by the computation of a new model obtained by applying
the maximum likelihood method to the last precise sample. These two steps are repeated until convergence
to a model is achieved.

In [CD16], it has been shown that the observation sample implicitly built at each step can be represented
by a probability distribution on the domain of the hidden variable that is in agreement with the observed
frequencies of the coarse data. It is obtained by applying, at each step of the procedure, the oldest (proba-
bilistic) revision rule well-known in AI, namely Jeffrey’s rule [Jef83], to the current best parametric model.
This form of revision considers a prior probability p(x, @) on the domain & of a variable X, and new in-
formation made of a probability distribution over a partition {A,,Az,...,A,} of & (representing the coarse
data). If D; is the “new” probability of A;, the old distribution p(x, @) is revised so as to be in agreement
with the new information. The revised probability function is of the form p’(x, 0) = ar DP; : p(x, O|A;).

17
The revision step minimally changes the prior probability function in the sense of Kullback-Leibler relative
entropy.

In the case of the EM algorithm, p; is the frequency of the coarse observation A;, and p(x, @) is the
current best parametric model. The distribution p’(x, @) corresponds to a new sample of X in agreement
with the coarse observation. In other words the EM algorithm in turn revises the parametric model to
make it consistent with the coarse data, and applies maximum entropy to the new obtained sample, thus
minimizing the relative (entropic) distance between a parametric model and a probability distribution in
agreement with the coarse data.

4.4 Conceptual Spaces and the Semantic Description of Vector Representations

Neural networks, and many other approaches in machine learning, crucially rely on vector representations.
Compared to symbolic representations, using vectors has many advantages (e.g., their continuous nature
often makes optimizing loss functions easier). At the same time, however, vector representations tend to
be difficult to interpret, which makes the models that rely on them hard to explain as well. Since this is
clearly problematic in many application contexts, there has been an increasing interest in techniques for
linking vector spaces to symbolic representations. The main underlying principles go back to the theory
of conceptual spaces [G&r00], which was proposed by Gardenfors as an intermediate representation level
between vector space representations and symbolic representations. Conceptual spaces are essentially
vector space models, as each object from the domain of discourse is represented as a vector, but they differ
in two crucial ways. First, the dimensions of a conceptual space usually correspond to interpretable salient
features. Second, (natural) properties and concepts are explicitly modelled as (convex) regions. Given a
conceptual space representation, we can thus, e.g., enumerate which properties are satisfied by a given
object, determine whether two concepts are disjoint or not, or rank objects according to a given (salient)
ordinal feature.

Conceptual spaces were proposed as a framework for studying cognitive and linguistic phenomena,
such as concept combination, metaphor and vagueness. As such, the problem of learning conceptual
spaces from data has not received much attention. Within a broader setting, however, several authors
have studied approaches for learning vector space representations that share important characteristics
with conceptual spaces. The main focus in this context has been on learning vector space models with
interpretable dimensions. For example, it has been proposed that non-negative matrix factorization leads
to representations with dimensions that are easier to interpret than those obtained with other matrix fac-
torization methods [LS99], especially when combined with sparseness constraints [Hoy04]. More recently,
a large number of neural network models have been proposed with the aim of learning vectors with inter-
pretable dimensions, under the umbrella term of disentangled representation learning [CDH*t 16, HMP*17].
Another possibility, advocated in [DS15], is to learn (non-orthogonal) directions that model interpretable
salient features within a vector space whose dimensions themselves may not be interpretable. Beyond
interpretable dimensions, some authors have also looked at modelling properties and concepts as regions
in a vector space. For example, [Erk09] proposed to learn region representations of word meaning. More
recent approaches along these lines include [VM15], where words are modelled as Gaussian distributions,
and [JS17], where word regions were learned using an ordinal regression model with a quadratic kernel.
Some authors have also looked at inducing region based representations of concepts from the vector rep-
resentations of known instances of these concepts [BS18, GBP18]. Finally, within a broader setting, some
approaches have been developed that link vectors to natural language descriptions, for instance linking
word vectors to dictionary definitions [HCKB16] or images to captions [KFF15].

The aforementioned approaches have found various applications. Within the specific context of ex-
plainable machine learning, at least two different strategies may be pursued. One possibility is to train a
model in the usual way (e.g., a neural network classifier), and then approximate this model based on the
semantic description of the vector representations involved. For instance, [AKS16] suggests a method for
learning a rule based classifier that describes a feedforward neural network. The second possibility is to
extract an interpretable (qualitative) representation from the vector space, e.g., by treating interpretable
dimensions as ordinal features, and then train a model on that interpretable representation [DS15].

18
