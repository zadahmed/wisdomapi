{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import io\n",
    "import string\n",
    "import requests\n",
    "import re\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from textblob import TextBlob\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_symbols = \"ΔΩπϴλθ°îĵk̂ûαβγδεζηθικλμνξοπρςστυφχψωΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩϴ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bytes stream of web pdf\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.09903.pdf\"\n",
    "#pdfurl = \"http://arxiv.org/pdf/1811.04422v1\"\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.09956\"\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.09412.pdf\"\n",
    "#pdfurl = \"http://arxiv.org/pdf/1411.6753v1\"\n",
    "pdfurl = \"https://arxiv.org/pdf/2001.10393.pdf\"\n",
    "r = requests.get(pdfurl, stream=True)\n",
    "f = io.BytesIO(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pdfminer\n",
    "rsrcmgr = PDFResourceManager()\n",
    "retstr = io.StringIO()\n",
    "codec = 'utf-8'\n",
    "laparams = LAParams()\n",
    "device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "pagenos=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract abstract\n",
    "content = []\n",
    "for page in PDFPage.get_pages(fp=f, pagenos=pagenos, maxpages=0, caching=True, check_extractable=True):\n",
    "    interpreter.process_page(page)\n",
    "\n",
    "text = retstr.getvalue()\n",
    "content.append(text)\n",
    "# close apps\n",
    "device.close()\n",
    "retstr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "splits = content[0].split(\"\\n\\n\")\n",
    "for chunk in splits:\n",
    "    # if ocr has picked up annoying numbers along side with many \"\\n\"\n",
    "    if chunk.count(\"\\n\") >= 3:\n",
    "        dummy = chunk.split(\"\\n\")\n",
    "        dummy_cnt = 0\n",
    "        for d in dummy:\n",
    "            if len(d)>1:\n",
    "                dummy_cnt += 1\n",
    "        if dummy_cnt > 2:\n",
    "            text.append(chunk)\n",
    "    else:\n",
    "        text.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify abstract\n",
    "if \"abstract\" in \" \".join(i for i in text).lower():\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        if \"abstract\" in text[cnt].lower():\n",
    "            if len(text[cnt].split()) < 2:\n",
    "                cnt += 1\n",
    "                break\n",
    "            else:\n",
    "                if re.match(\"^abstract\", text[cnt][:8].lower()):\n",
    "                    text[cnt] = text[cnt][8:]\n",
    "                break\n",
    "        cnt += 1\n",
    "    text = text[cnt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify references\n",
    "cnt = 0\n",
    "while True:\n",
    "    if \"References\" in text[cnt] or \"Appendix\" in text[cnt] or \"Bibliography\" in text[cnt] or \"REFERENCES\" in text[cnt] or \"acknowledgements\" in text[cnt] or \"ACKNOWLEDGEMENTS\" in text[cnt]:\n",
    "        if len(text[cnt].split()) < 3:\n",
    "            break\n",
    "        if \"\\n\" in text[cnt]:\n",
    "            break\n",
    "    cnt += 1\n",
    "\n",
    "text = text[:cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove equation number references\n",
    "clean1 = []\n",
    "for t in text:\n",
    "    if len(t.split()) == 1:\n",
    "        if re.match(\"^\\(\\d\\)\", t):\n",
    "            pass\n",
    "        if re.match(\"\\d\", t):\n",
    "            pass\n",
    "    elif len(t.split()) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        clean1.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove headers\n",
    "clean2 = []\n",
    "cnt = 0\n",
    "while cnt < len(clean1):\n",
    "    if len(clean1[cnt].split()) < 10:\n",
    "        dummy = re.sub(\"\\d\", \"\", clean1[cnt])\n",
    "        dummy = dummy.strip()\n",
    "        if len(dummy.split()) <= 1:\n",
    "            pass\n",
    "        elif dummy[-1] not in string.punctuation:\n",
    "            dummy2 = re.sub(\"\\d\", \"\", clean1[cnt+1])\n",
    "            dummy2 = re.sub('[^\\w\\s]','', dummy2)\n",
    "            dummy2 = dummy2.strip()\n",
    "            if dummy2:\n",
    "                if dummy2[0].isupper():\n",
    "                    pass\n",
    "                else:\n",
    "                    clean2.append(clean1[cnt])\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            clean2.append(clean1[cnt])\n",
    "    else:\n",
    "        clean2.append(clean1[cnt])\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove figure captions\n",
    "clean3 = []\n",
    "cnt = 0\n",
    "while cnt < len(clean2):\n",
    "    if re.match('^Fig', clean2[cnt]) or re.match('^fig.', clean2[cnt].lower()) :\n",
    "        pass\n",
    "    else:\n",
    "        clean3.append(clean2[cnt])\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove table data\n",
    "clean4 = []\n",
    "cnt = 0\n",
    "while cnt < len(clean3)-1:\n",
    "    if clean3[cnt][-1] == \".\":\n",
    "        if re.sub(\"\\d\", \"\", clean3[cnt+1]).strip()[0].islower():\n",
    "            pass\n",
    "        else:\n",
    "            clean4.append(clean3[cnt])\n",
    "    elif clean3[cnt][-1] == \"-\" or clean3[cnt][-1] == \"-\":\n",
    "        if clean3[cnt+1][0].islower():\n",
    "            clean4.append(clean3[cnt])\n",
    "        else:\n",
    "            pass\n",
    "    elif \"©\" in clean3[cnt]:\n",
    "        pass\n",
    "    else:\n",
    "        clean4.append(clean3[cnt])\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove citations\n",
    "clean5 = \" \".join(c for c in clean4)\n",
    "clean5 = re.sub(\"^\\[\\d\\]\", \"\", clean5)\n",
    "clean5 = re.sub(\"^\\[\\d\\d\\]\", \"\", clean5)\n",
    "clean5 = re.sub(\"^\\[\\d\\d\\]\", \"\", clean5)\n",
    "clean5 = re.sub(\"-\\n\", \"\", clean5)\n",
    "clean5 = re.sub(\"\\n\", \" \", clean5)\n",
    "clean5 = re.sub(\"  \", \" \", clean5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarisation of top 5 key points\n",
    "key_points = 5\n",
    "summary = []\n",
    "blob = TextBlob(clean5)\n",
    "sentences = [str(sentence) for sentence in blob.sentences]\n",
    "for sentence in sentences:\n",
    "    if sentence.find(\":\", 0, 1) != -1 and sentence.find(\"-\", 1, 3) != -1:\n",
    "        pass\n",
    "    else:\n",
    "        if len(sentence)>2:\n",
    "            if len(sentence.split()) < 150:\n",
    "                summary.append(sentence)\n",
    "parser = PlaintextParser.from_string(' '.join(str(sentence) for sentence in summary), Tokenizer(\"english\"))\n",
    "summarizer = TextRankSummarizer()\n",
    "doc_summary = summarizer(parser.document, key_points)\n",
    "doc_summary = [str(sentence) for sentence in doc_summary]\n",
    "for sent in doc_summary:\n",
    "    print(sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wisdomaiengine import pdfdocumentextracter, summarisepdfdocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfurl = \"https://arxiv.org/pdf/2001.09903.pdf\"\n",
    "#pdfurl = \"http://arxiv.org/pdf/1811.04422v1\"\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.09956\"\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.09412.pdf\"\n",
    "#pdfurl = \"http://arxiv.org/pdf/1411.6753v1\"\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.10393.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdfdocumentextracter(pdfurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the 2013 and 2014 episodes, which had the best Swift sampling, we ﬁnd that the correlation between the UV/optical and X-ray ﬂuxes is signiﬁcantly steeper during the decay (soft state) of the outburst than during the rise (hard-to-soft state). \n",
      "\n",
      "For each of our selected observations, we obtained the X-ray spectra and the associated response ﬁles using the online XRT data products tool1, which uses the latest version of the Swift software and calibration (Evans et al. 2009). \n",
      "\n",
      "We searched the Swift data archive for outbursts of Aql X-1 that had i) good sampling of the entire outburst with the X-ray Telescope (XRT) and ii) observations in one or more UV ﬁlters on the UltraViolet and Optical Telescope (UVOT) consistently taken along the entire outburst. \n",
      "\n",
      "To have some sense of the X-ray spectral evolution along the outbursts, we include the daily-averaged 15–50 keV light curves obtained from the Swift/BAT transient monitor project (Krimm et al. 2013) and the changes in the hardness ratio (HR) in Figure 1. \n",
      "\n",
      "We see that the BAT 15–50 keV peak occurs during the rise of the three outbursts, after which there is a drop of the BAT ﬂux that we can identify with the source entering the soft state. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = summarisepdfdocument(text)\n",
    "for i in summary:\n",
    "     print(i, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
