{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import io\n",
    "import string\n",
    "import requests\n",
    "import re\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bytes stream of web pdf\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.09903.pdf\"\n",
    "pdfurl = \"http://arxiv.org/pdf/1811.04422v1\"\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.09956\"\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.09412.pdf\"\n",
    "#pdfurl = \"http://arxiv.org/pdf/1411.6753v1\"\n",
    "#pdfurl = \"https://arxiv.org/pdf/2001.10393.pdf\"\n",
    "r = requests.get(pdfurl, stream=True)\n",
    "f = io.BytesIO(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pdfminer\n",
    "rsrcmgr = PDFResourceManager()\n",
    "retstr = io.StringIO()\n",
    "codec = 'utf-8'\n",
    "laparams = LAParams()\n",
    "device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "pagenos=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# extract abstract\n",
    "content = []\n",
    "cnt = 1\n",
    "for page in PDFPage.get_pages(fp=f, pagenos=pagenos, maxpages=0, caching=True, check_extractable=True):\n",
    "    interpreter.process_page(page)\n",
    "    print(cnt)\n",
    "    cnt += 1\n",
    "\n",
    "text = retstr.getvalue()\n",
    "content.append(text)\n",
    "# close apps\n",
    "device.close()\n",
    "retstr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "splits = content[0].split(\"\\n\\n\")\n",
    "for chunk in splits:\n",
    "    # if ocr has picked up annoying numbers along side with many \"\\n\"\n",
    "    if chunk.count(\"\\n\") >= 3:\n",
    "        dummy = chunk.split(\"\\n\")\n",
    "        dummy_cnt = 0\n",
    "        for d in dummy:\n",
    "            if len(d)>1:\n",
    "                dummy_cnt += 1\n",
    "        if dummy_cnt > 2:\n",
    "            text.append(chunk)\n",
    "    else:\n",
    "        text.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify abstract\n",
    "cnt = 0\n",
    "while True:\n",
    "    if \"abstract\" in text[cnt].lower():\n",
    "        if len(text[cnt].split()) < 2:\n",
    "            cnt += 1\n",
    "            break\n",
    "        else:\n",
    "            cnt -= 1\n",
    "    cnt += 1\n",
    "\n",
    "text = text[cnt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify references\n",
    "cnt = 0\n",
    "while True:\n",
    "    if \"References\" in text[cnt] or \"Appendix\" in text[cnt] or \"Bibliography\" in text[cnt]:\n",
    "        if len(text[cnt].split()) < 3:\n",
    "            break\n",
    "    cnt += 1\n",
    "\n",
    "text = text[:cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove equation number references\n",
    "clean1 = []\n",
    "for t in text:\n",
    "    if len(t.split()) == 1:\n",
    "        if re.match(\"^\\(\\d\\)\", t):\n",
    "            pass\n",
    "        if re.match(\"\\d\", t):\n",
    "            pass\n",
    "    elif len(t.split()) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        clean1.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove headers\n",
    "clean2 = []\n",
    "cnt = 0\n",
    "while cnt < len(clean1):\n",
    "    if len(clean1[cnt].split()) < 10:\n",
    "        dummy = re.sub(\"\\d\", \"\", clean1[cnt])\n",
    "        dummy = dummy.strip()\n",
    "        if dummy[-1] not in string.punctuation:\n",
    "            dummy2 = re.sub(\"\\d\", \"\", clean1[cnt+1])\n",
    "            dummy2 = dummy2.strip()\n",
    "            if dummy2[0].isupper():\n",
    "                pass\n",
    "            else:\n",
    "                clean2.append(clean1[cnt])\n",
    "        else:\n",
    "            clean2.append(clean1[cnt])\n",
    "    else:\n",
    "        clean2.append(clean1[cnt])\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove citations\n",
    "clean3 = \" \".join(c for c in clean2)\n",
    "clean3 = re.sub(\"^\\[\\d\\]\", \"\", clean3)\n",
    "clean3 = re.sub(\"^\\[\\d\\d\\]\", \"\", clean3)\n",
    "clean3 = re.sub(\"^\\[\\d\\d\\]\", \"\", clean3)\n",
    "clean3 = re.sub(\"\\n\", \" \", clean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning. Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of the independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial machine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs: adversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be. I will focus on deterministic discrete-time optimal control because it matches many existing adversarial attacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too. The system to be controlled is called the plant, which is deﬁned by the system dynamics: where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint set. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0 to T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running cost: which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon: xt+1 = f (xt, ut) gt(xt, ut) gT (xT ) which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1 in order to minimize the objective: u0...uT −1 gT (xT ) + gt(xt, ut) T −1 xt+1 = f (xt, ut), ut ∈ Ut, ∀t x0 given \\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed states to inputs. In optimal control the dynamics f is known to the controller. There are two styles of solutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known, the problem becomes either robust control where control is carried out in a minimax fashion to accommodate the worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23]. Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning studies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data poisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to control the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be hard to detect. Unfortunately, the notations from the control community and the machine learning community clash. For example, x denotes the state in control but the feature vector in machine learning. I will use the machine learning convention below. In training-data poisoning the adversary can modify the training data. The machine learner then trains a “wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some nefarious purpose. I use supervised learning for illustration. At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine learner performs batch learning, then the adversary has a degenerate one-step control problem. One-step control has not been the focus of the control community and there may not be ample algorithmic solutions to borrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support Vector Machine (SVM) with a batch training set as an example below: • The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by a weight vector w. I will use h and w interchangeably. • The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n. • The control constraint set U0 consists of training sets available to the adversary; if the adversary can arbitrary modify a training set for supervised learning (including changing features and labels, inserting and deleting items), this could be U0 = ∪∞ n=0(X × Y)n, namely all training sets of all sizes. This is a large control space. • The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this would be empirical risk minimization with hinge loss ℓ() and a regularizer: w1 = f (u0) ∈ argminw ℓ(w, xi, yi) + λkwk2. The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics f () if it knows the form (5), ℓ(), and the value of λ. • The time horizon T = 1. \\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0. This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of g0(u0) = distance(u0, ˜u). The running cost is domain dependent. For example, the distance function may count the number of modiﬁed training items; or sum up the Euclidean distance of changes in feature vectors. • The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also domain dependent. For example: – If the adversary must force the learner into exactly arriving at some target model w∗, then g1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard constraint. – If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm. – If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive, it can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally, W∗ can be a polytope deﬁned by multiple future classiﬁcation constraints. With these deﬁnitions, the adversary’s one-step control problem (4) specializes to g1(w1) + g0(w0, u0) w1 = f (w0, u0) Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level optimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi- soning [12, 21]. The adversary performs classic discrete-time control if the learner is sequential: • The learner starts from an initial model w0, which is the initial state. • The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . . • The dynamics is the sequential update algorithm of the learner. For example, the learner may perform one step of gradient descent: wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt). • The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it could measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence ˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence. • The adversary’s terminal cost gT (wT ) is the same as in the batch case. The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential teaching can be found in [1, 18, 19]. Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already- trained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the following one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine learning model classiﬁes x and x′ diﬀerently. That is, distance(x, x′) h(x) 6= h(y). The distance function is domain-dependent, though in practice the adversary often uses a mathematically convenient surrogate such as some p-norm kx − x′kp. One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and the adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against image classiﬁcation: • Let the initial state x0 = x be the clean image. • The adversary’s control input u0 is the vector of pixel value changes. • The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid pixel values (assumed to be normalized in [0, 1]). • The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0. • The adversary’s running cost is g0(x0, u0) = distance(x0, x1). • The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed. With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack problem (9). This control view on test-time attack is more interesting when the adversary’s actions are sequential U0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running cost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control view is that the action cost is assumed to be additive over the steps. Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to require the learned model h to have the large-margin property with respect to a training set. Let (x, y) be any training item, and ǫ a margin parameter. Then the large-margin property states that the decision boundary induced by h should not pass ǫ-close to (x, y): This is an uncountable number of constraints. SVMs, but impractical otherwise. ∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y. It is relatively easy to enforce for linear learners such as Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with a ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd an adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint h(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning algorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats for k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k. It should be clear that such defense is similar to training-data poisoning, in that the defender uses data to modify the learned model. This is especially interesting when the learner performs sequential updates. One way to formulate adversarial training defense as control is the following: \\x0c• The state is the model ht. Initially h0 can be the model trained on the original training data. • The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X× y. • The dynamics ht+1 = f (ht, ut) is one-step update of the model, e.g. by back-propagation. • The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running • The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the cost sums up to k). original training data. Of course, the resulting control problem (4) does not directly utilize adversarial examples. One way to incorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on ht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks. When adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce- ment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary. The adversary may do so by manipulating the rewards and the states experienced by the learner [11, 14]. To simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit, because this does not involve deception through perceived states. To review, in stochastic multi-armed bandit the learner at iteration t chooses one of k arms, denoted by It ∈ [k], to pull according to some strategy [6]. For example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm Ti(t − 1)(cid:19) where Ti(t − 1) is the number of times arm i has been pulled up to time t − 1, ˆµi,Ti(t−1) is the empirical mean of arm i so far, and ψ∗ is the dual of a convex function ψ. The environment generates a stochastic reward rIt ∼ νIt . The learner updates its estimate of the pulled arm: ˆµIt,TIt (t) = TIt(t − 1) + 1 which in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the t=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit pseudo-regret T µmax − EPT strategies oﬀer upper bounds on the pseudo-regret. With adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the environmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into with some ut ∈ R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal reward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may want the learner to frequently pull a particular target arm i∗ ∈ [k]. It should be noted that the adversary’s goal may not be the exact opposite of the learner’s goal: the target arm i∗ is not necessarily the one with the worst mean reward, and the adversary may not seek pseudo-regret maximization. Adversarial reward shaping can be formulated as stochastic optimal control: • The state st, now called control state to avoid confusion with the Markov Decision Process states experienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t: st = (T1(t − 1), ˆµ1,T1(t−1), . . . , Tk(t − 1), ˆµk,Tk(t−1), It). rIt + ut \\x0c• The control input is ut ∈ Ut with Ut = R in the unconstrained shaping case, or the appropriate Ut if the rewards must be binary, for example. • The dynamics st+1 = f (st, ut) is straightforward via empirical mean update (12), TIt increment, and new arm choice (11). t. For instance, • The adversary’s running cost gt(st, ut) reﬂects shaping eﬀort and target arm achievement in iteration gt(st, ut) = u2 t + Iλ[It 6= i∗]. where λ > 0 is a trade oﬀ parameter. • There is not necessarily a time horizon T or a terminal cost gT (sT ). The control state is stochastic due to the stochastic reward rIt entering through (12). There are a number of potential beneﬁts in taking the optimal control view: • It oﬀers a uniﬁed conceptual framework for adversarial machine learning; • The optimal control literature provides eﬃcient solutions when the dynamics f is known and one can take the continuous limit to solve the diﬀerential equations [15]; • Reinforcement learning, either model-based with coarse system identiﬁcation or model-free policy it- eration, allows approximate optimal control when f is unknown, as long as the adversary can probe the dynamics [8, 9]; • A generic defense strategy may be to limit the controllability the adversary has over the learner. • I mention in passing that the optimal control view applies equally to machine teaching [27, 29], and thus extends to the application of personalized education [22, 24]. I need to point out some limitations: • Having a uniﬁed optimal control view does not automatically produce eﬃcient solutions to the control problem (4). For adversarial machine learning applications the dynamics f is usually highly nonlinear and complex. Furthermore, in graybox and blackbox attack settings f is not fully known to the attacker. They aﬀect the complexity in ﬁnding an optimal control. • The adversarial learning setting is largely non-game theoretic, though there are exceptions [5, 16]. These problems call for future research from both machine learning and control communities. Acknowledgments. I acknowledge funding NSF 1837132, 1545481, 1704117, 1623605, 1561512, and the MADLab AF Center of Excellence FA9550-18-1-0166.'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='[23]'>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(\"^\\[\\d\\d\\]\", \"[23]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = ' '\n",
    "len(thing.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c3.2 Test-Time Attack',\n",
       " 'Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,',\n",
       " 'min',\n",
       " '′',\n",
       " 'x',\n",
       " 's.t.',\n",
       " 'distance(x, x′)',\n",
       " 'h(x) 6= h(y).',\n",
       " '(9)',\n",
       " 'The distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm kx − x′kp.',\n",
       " 'One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:',\n",
       " '• Let the initial state x0 = x be the clean image.',\n",
       " '• The adversary’s control input u0 is the vector of pixel value changes.',\n",
       " '• The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid',\n",
       " 'pixel values (assumed to be normalized in [0, 1]).',\n",
       " '• The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0.',\n",
       " '• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).',\n",
       " '• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is',\n",
       " 'only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.',\n",
       " 'With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).',\n",
       " 'This control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.',\n",
       " '3.3 Defense Against Test-Time Attack by Adversarial Training',\n",
       " 'Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):',\n",
       " 'This is an uncountable number of constraints.\\nSVMs, but impractical otherwise.',\n",
       " '∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y.',\n",
       " '(10)',\n",
       " 'It is relatively easy to enforce for linear learners such as',\n",
       " 'Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.',\n",
       " 'It should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:',\n",
       " '4',\n",
       " '\\x0c• The state is the model ht. Initially h0 can be the model trained on the original training data.',\n",
       " '• The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X× y.',\n",
       " '• The dynamics ht+1 = f (ht, ut) is one-step update of the model, e.g. by back-propagation.',\n",
       " '• The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running',\n",
       " '• The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the',\n",
       " 'cost sums up to k).',\n",
       " 'original training data.',\n",
       " 'Of course, the resulting control problem (4) does not directly utilize adversarial examples. One way to\\nincorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on\\nht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks.',\n",
       " '3.4 Adversarial Reward Shaping',\n",
       " 'When adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce-\\nment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary.\\nThe adversary may do so by manipulating the rewards and the states experienced by the learner [11, 14].',\n",
       " 'To simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit,\\nbecause this does not involve deception through perceived states. To review, in stochastic multi-armed bandit\\nthe learner at iteration t chooses one of k arms, denoted by It ∈ [k], to pull according to some strategy [6].\\nFor example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm',\n",
       " 'It ∈ argmaxi∈[k] ˆµi,Ti(t−1) + ψ∗−1(cid:18) α log t',\n",
       " 'Ti(t − 1)(cid:19)',\n",
       " 'where Ti(t − 1) is the number of times arm i has been pulled up to time t − 1, ˆµi,Ti(t−1) is the empirical\\nmean of arm i so far, and ψ∗ is the dual of a convex function ψ. The environment generates a stochastic\\nreward rIt ∼ νIt . The learner updates its estimate of the pulled arm:',\n",
       " '(11)',\n",
       " '(12)',\n",
       " 'ˆµIt,TIt (t) =',\n",
       " 'ˆµIt,TIt (t−1)TIt (t − 1) + rIt',\n",
       " 'TIt(t − 1) + 1',\n",
       " 'which in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the\\nt=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit',\n",
       " 'pseudo-regret T µmax − EPT',\n",
       " 'strategies oﬀer upper bounds on the pseudo-regret.',\n",
       " 'With adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the',\n",
       " 'environmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into',\n",
       " 'with some ut ∈ R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal\\nreward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may\\nwant the learner to frequently pull a particular target arm i∗ ∈ [k]. It should be noted that the adversary’s\\ngoal may not be the exact opposite of the learner’s goal: the target arm i∗ is not necessarily the one with\\nthe worst mean reward, and the adversary may not seek pseudo-regret maximization.',\n",
       " 'Adversarial reward shaping can be formulated as stochastic optimal control:',\n",
       " '• The state st, now called control state to avoid confusion with the Markov Decision Process states',\n",
       " 'experienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t:',\n",
       " 'st = (T1(t − 1), ˆµ1,T1(t−1), . . . , Tk(t − 1), ˆµk,Tk(t−1), It).',\n",
       " 'rIt + ut',\n",
       " '5',\n",
       " '\\x0c• The control input is ut ∈ Ut with Ut = R in the unconstrained shaping case, or the appropriate Ut if',\n",
       " 'the rewards must be binary, for example.',\n",
       " '• The dynamics st+1 = f (st, ut) is straightforward via empirical mean update (12), TIt increment, and',\n",
       " 'new arm choice (11).',\n",
       " 't. For instance,',\n",
       " '• The adversary’s running cost gt(st, ut) reﬂects shaping eﬀort and target arm achievement in iteration',\n",
       " 'gt(st, ut) = u2',\n",
       " 't + Iλ[It 6= i∗].',\n",
       " '(13)',\n",
       " 'where λ > 0 is a trade oﬀ parameter.',\n",
       " '• There is not necessarily a time horizon T or a terminal cost gT (sT ).',\n",
       " 'The control state is stochastic due to the stochastic reward rIt entering through (12).',\n",
       " '4 Advantages of the Optimal Control View',\n",
       " 'There are a number of potential beneﬁts in taking the optimal control view:',\n",
       " '• It oﬀers a uniﬁed conceptual framework for adversarial machine learning;',\n",
       " '• The optimal control literature provides eﬃcient solutions when the dynamics f is known and one can',\n",
       " 'take the continuous limit to solve the diﬀerential equations [15];',\n",
       " '• Reinforcement learning, either model-based with coarse system identiﬁcation or model-free policy it-\\neration, allows approximate optimal control when f is unknown, as long as the adversary can probe\\nthe dynamics [8, 9];',\n",
       " '• A generic defense strategy may be to limit the controllability the adversary has over the learner.',\n",
       " '• I mention in passing that the optimal control view applies equally to machine teaching [27, 29], and',\n",
       " 'thus extends to the application of personalized education [22, 24].',\n",
       " 'I need to point out some limitations:',\n",
       " '• Having a uniﬁed optimal control view does not automatically produce eﬃcient solutions to the control\\nproblem (4). For adversarial machine learning applications the dynamics f is usually highly nonlinear\\nand complex. Furthermore, in graybox and blackbox attack settings f is not fully known to the\\nattacker. They aﬀect the complexity in ﬁnding an optimal control.',\n",
       " '• The adversarial learning setting is largely non-game theoretic, though there are exceptions [5, 16].',\n",
       " 'These problems call for future research from both machine learning and control communities.',\n",
       " 'Acknowledgments. I acknowledge funding NSF 1837132, 1545481, 1704117, 1623605, 1561512, and',\n",
       " 'the MADLab AF Center of Excellence FA9550-18-1-0166.']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c3.2 Test-Time Attack',\n",
       " 'Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,',\n",
       " 'min',\n",
       " '′',\n",
       " 'x',\n",
       " 's.t.',\n",
       " 'distance(x, x′)',\n",
       " 'h(x) 6= h(y).',\n",
       " '(9)',\n",
       " 'The distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm kx − x′kp.',\n",
       " 'One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:',\n",
       " '• Let the initial state x0 = x be the clean image.',\n",
       " '• The adversary’s control input u0 is the vector of pixel value changes.',\n",
       " '• The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid',\n",
       " 'pixel values (assumed to be normalized in [0, 1]).',\n",
       " '• The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0.',\n",
       " '• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).',\n",
       " '• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is',\n",
       " 'only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.',\n",
       " 'With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).',\n",
       " 'This control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.',\n",
       " '3.3 Defense Against Test-Time Attack by Adversarial Training',\n",
       " 'Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):',\n",
       " 'This is an uncountable number of constraints.\\nSVMs, but impractical otherwise.',\n",
       " '∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y.',\n",
       " '(10)',\n",
       " 'It is relatively easy to enforce for linear learners such as',\n",
       " 'Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.',\n",
       " 'It should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:',\n",
       " '4',\n",
       " '\\x0c• The state is the model ht. Initially h0 can be the model trained on the original training data.',\n",
       " '• The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X× y.',\n",
       " '• The dynamics ht+1 = f (ht, ut) is one-step update of the model, e.g. by back-propagation.',\n",
       " '• The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running',\n",
       " '• The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the',\n",
       " 'cost sums up to k).',\n",
       " 'original training data.',\n",
       " 'Of course, the resulting control problem (4) does not directly utilize adversarial examples. One way to\\nincorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on\\nht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks.',\n",
       " '3.4 Adversarial Reward Shaping',\n",
       " 'When adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce-\\nment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary.\\nThe adversary may do so by manipulating the rewards and the states experienced by the learner [11, 14].',\n",
       " 'To simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit,\\nbecause this does not involve deception through perceived states. To review, in stochastic multi-armed bandit\\nthe learner at iteration t chooses one of k arms, denoted by It ∈ [k], to pull according to some strategy [6].\\nFor example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm',\n",
       " 'It ∈ argmaxi∈[k] ˆµi,Ti(t−1) + ψ∗−1(cid:18) α log t',\n",
       " 'Ti(t − 1)(cid:19)',\n",
       " 'where Ti(t − 1) is the number of times arm i has been pulled up to time t − 1, ˆµi,Ti(t−1) is the empirical\\nmean of arm i so far, and ψ∗ is the dual of a convex function ψ. The environment generates a stochastic\\nreward rIt ∼ νIt . The learner updates its estimate of the pulled arm:',\n",
       " '(11)',\n",
       " '(12)',\n",
       " 'ˆµIt,TIt (t) =',\n",
       " 'ˆµIt,TIt (t−1)TIt (t − 1) + rIt',\n",
       " 'TIt(t − 1) + 1',\n",
       " 'which in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the\\nt=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit',\n",
       " 'pseudo-regret T µmax − EPT',\n",
       " 'strategies oﬀer upper bounds on the pseudo-regret.',\n",
       " 'With adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the',\n",
       " 'environmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into',\n",
       " 'with some ut ∈ R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal\\nreward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may\\nwant the learner to frequently pull a particular target arm i∗ ∈ [k]. It should be noted that the adversary’s\\ngoal may not be the exact opposite of the learner’s goal: the target arm i∗ is not necessarily the one with\\nthe worst mean reward, and the adversary may not seek pseudo-regret maximization.',\n",
       " 'Adversarial reward shaping can be formulated as stochastic optimal control:',\n",
       " '• The state st, now called control state to avoid confusion with the Markov Decision Process states',\n",
       " 'experienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t:',\n",
       " 'st = (T1(t − 1), ˆµ1,T1(t−1), . . . , Tk(t − 1), ˆµk,Tk(t−1), It).',\n",
       " 'rIt + ut',\n",
       " '5',\n",
       " '\\x0c• The control input is ut ∈ Ut with Ut = R in the unconstrained shaping case, or the appropriate Ut if',\n",
       " 'the rewards must be binary, for example.',\n",
       " '• The dynamics st+1 = f (st, ut) is straightforward via empirical mean update (12), TIt increment, and',\n",
       " 'new arm choice (11).',\n",
       " 't. For instance,',\n",
       " '• The adversary’s running cost gt(st, ut) reﬂects shaping eﬀort and target arm achievement in iteration',\n",
       " 'gt(st, ut) = u2',\n",
       " 't + Iλ[It 6= i∗].',\n",
       " '(13)',\n",
       " 'where λ > 0 is a trade oﬀ parameter.',\n",
       " '• There is not necessarily a time horizon T or a terminal cost gT (sT ).',\n",
       " 'The control state is stochastic due to the stochastic reward rIt entering through (12).',\n",
       " '4 Advantages of the Optimal Control View',\n",
       " 'There are a number of potential beneﬁts in taking the optimal control view:',\n",
       " '• It oﬀers a uniﬁed conceptual framework for adversarial machine learning;',\n",
       " '• The optimal control literature provides eﬃcient solutions when the dynamics f is known and one can',\n",
       " 'take the continuous limit to solve the diﬀerential equations [15];',\n",
       " '• Reinforcement learning, either model-based with coarse system identiﬁcation or model-free policy it-\\neration, allows approximate optimal control when f is unknown, as long as the adversary can probe\\nthe dynamics [8, 9];',\n",
       " '• A generic defense strategy may be to limit the controllability the adversary has over the learner.',\n",
       " '• I mention in passing that the optimal control view applies equally to machine teaching [27, 29], and',\n",
       " 'thus extends to the application of personalized education [22, 24].',\n",
       " 'I need to point out some limitations:',\n",
       " '• Having a uniﬁed optimal control view does not automatically produce eﬃcient solutions to the control\\nproblem (4). For adversarial machine learning applications the dynamics f is usually highly nonlinear\\nand complex. Furthermore, in graybox and blackbox attack settings f is not fully known to the\\nattacker. They aﬀect the complexity in ﬁnding an optimal control.',\n",
       " '• The adversarial learning setting is largely non-game theoretic, though there are exceptions [5, 16].',\n",
       " 'These problems call for future research from both machine learning and control communities.',\n",
       " 'Acknowledgments. I acknowledge funding NSF 1837132, 1545481, 1704117, 1623605, 1561512, and',\n",
       " 'the MADLab AF Center of Excellence FA9550-18-1-0166.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8\\n1\\n0\\n2',\n",
       " ' ',\n",
       " 'v\\no\\nN\\n1\\n1',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " 'G\\nL\\n.\\ns\\nc\\n[\\n \\n ',\n",
       " '1\\nv\\n2\\n2\\n4\\n4\\n0',\n",
       " '.',\n",
       " '1\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c3.2 Test-Time Attack',\n",
       " 'Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,',\n",
       " 'min',\n",
       " '′',\n",
       " 'x',\n",
       " 's.t.',\n",
       " 'distance(x, x′)',\n",
       " 'h(x) 6= h(y).',\n",
       " '(9)',\n",
       " 'The distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm kx − x′kp.',\n",
       " 'One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:',\n",
       " '• Let the initial state x0 = x be the clean image.',\n",
       " '• The adversary’s control input u0 is the vector of pixel value changes.',\n",
       " '• The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid',\n",
       " 'pixel values (assumed to be normalized in [0, 1]).',\n",
       " '• The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0.',\n",
       " '• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).',\n",
       " '• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is',\n",
       " 'only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.',\n",
       " 'With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).',\n",
       " 'This control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.',\n",
       " '3.3 Defense Against Test-Time Attack by Adversarial Training',\n",
       " 'Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):',\n",
       " 'This is an uncountable number of constraints.\\nSVMs, but impractical otherwise.',\n",
       " '∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y.',\n",
       " '(10)',\n",
       " 'It is relatively easy to enforce for linear learners such as',\n",
       " 'Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.',\n",
       " 'It should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:',\n",
       " '4',\n",
       " '\\x0c• The state is the model ht. Initially h0 can be the model trained on the original training data.',\n",
       " '• The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X× y.',\n",
       " '• The dynamics ht+1 = f (ht, ut) is one-step update of the model, e.g. by back-propagation.',\n",
       " '• The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running',\n",
       " '• The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the',\n",
       " 'cost sums up to k).',\n",
       " 'original training data.',\n",
       " 'Of course, the resulting control problem (4) does not directly utilize adversarial examples. One way to\\nincorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on\\nht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks.',\n",
       " '3.4 Adversarial Reward Shaping',\n",
       " 'When adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce-\\nment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary.\\nThe adversary may do so by manipulating the rewards and the states experienced by the learner [11, 14].',\n",
       " 'To simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit,\\nbecause this does not involve deception through perceived states. To review, in stochastic multi-armed bandit\\nthe learner at iteration t chooses one of k arms, denoted by It ∈ [k], to pull according to some strategy [6].\\nFor example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm',\n",
       " 'It ∈ argmaxi∈[k] ˆµi,Ti(t−1) + ψ∗−1(cid:18) α log t',\n",
       " 'Ti(t − 1)(cid:19)',\n",
       " 'where Ti(t − 1) is the number of times arm i has been pulled up to time t − 1, ˆµi,Ti(t−1) is the empirical\\nmean of arm i so far, and ψ∗ is the dual of a convex function ψ. The environment generates a stochastic\\nreward rIt ∼ νIt . The learner updates its estimate of the pulled arm:',\n",
       " '(11)',\n",
       " '(12)',\n",
       " 'ˆµIt,TIt (t) =',\n",
       " 'ˆµIt,TIt (t−1)TIt (t − 1) + rIt',\n",
       " 'TIt(t − 1) + 1',\n",
       " 'which in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the\\nt=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit',\n",
       " 'pseudo-regret T µmax − EPT',\n",
       " 'strategies oﬀer upper bounds on the pseudo-regret.',\n",
       " 'With adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the',\n",
       " 'environmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into',\n",
       " 'with some ut ∈ R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal\\nreward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may\\nwant the learner to frequently pull a particular target arm i∗ ∈ [k]. It should be noted that the adversary’s\\ngoal may not be the exact opposite of the learner’s goal: the target arm i∗ is not necessarily the one with\\nthe worst mean reward, and the adversary may not seek pseudo-regret maximization.',\n",
       " 'Adversarial reward shaping can be formulated as stochastic optimal control:',\n",
       " '• The state st, now called control state to avoid confusion with the Markov Decision Process states',\n",
       " 'experienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t:',\n",
       " 'st = (T1(t − 1), ˆµ1,T1(t−1), . . . , Tk(t − 1), ˆµk,Tk(t−1), It).',\n",
       " 'rIt + ut',\n",
       " '5',\n",
       " '\\x0c• The control input is ut ∈ Ut with Ut = R in the unconstrained shaping case, or the appropriate Ut if',\n",
       " 'the rewards must be binary, for example.',\n",
       " '• The dynamics st+1 = f (st, ut) is straightforward via empirical mean update (12), TIt increment, and',\n",
       " 'new arm choice (11).',\n",
       " 't. For instance,',\n",
       " '• The adversary’s running cost gt(st, ut) reﬂects shaping eﬀort and target arm achievement in iteration',\n",
       " 'gt(st, ut) = u2',\n",
       " 't + Iλ[It 6= i∗].',\n",
       " '(13)',\n",
       " 'where λ > 0 is a trade oﬀ parameter.',\n",
       " '• There is not necessarily a time horizon T or a terminal cost gT (sT ).',\n",
       " 'The control state is stochastic due to the stochastic reward rIt entering through (12).',\n",
       " '4 Advantages of the Optimal Control View',\n",
       " 'There are a number of potential beneﬁts in taking the optimal control view:',\n",
       " '• It oﬀers a uniﬁed conceptual framework for adversarial machine learning;',\n",
       " '• The optimal control literature provides eﬃcient solutions when the dynamics f is known and one can',\n",
       " 'take the continuous limit to solve the diﬀerential equations [15];',\n",
       " '• Reinforcement learning, either model-based with coarse system identiﬁcation or model-free policy it-\\neration, allows approximate optimal control when f is unknown, as long as the adversary can probe\\nthe dynamics [8, 9];',\n",
       " '• A generic defense strategy may be to limit the controllability the adversary has over the learner.',\n",
       " '• I mention in passing that the optimal control view applies equally to machine teaching [27, 29], and',\n",
       " 'thus extends to the application of personalized education [22, 24].',\n",
       " 'I need to point out some limitations:',\n",
       " '• Having a uniﬁed optimal control view does not automatically produce eﬃcient solutions to the control\\nproblem (4). For adversarial machine learning applications the dynamics f is usually highly nonlinear\\nand complex. Furthermore, in graybox and blackbox attack settings f is not fully known to the\\nattacker. They aﬀect the complexity in ﬁnding an optimal control.',\n",
       " '• The adversarial learning setting is largely non-game theoretic, though there are exceptions [5, 16].',\n",
       " 'These problems call for future research from both machine learning and control communities.',\n",
       " 'Acknowledgments. I acknowledge funding NSF 1837132, 1545481, 1704117, 1623605, 1561512, and',\n",
       " 'the MADLab AF Center of Excellence FA9550-18-1-0166.',\n",
       " 'References',\n",
       " '[1] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In',\n",
       " 'The Thirtieth AAAI Conference on Artiﬁcial Intelligence (AAAI-16), 2016.',\n",
       " '[2] Michael Athans and Peter L Falb. Optimal control: An introduction to the theory and its applications.',\n",
       " 'Courier Corporation, 2013.',\n",
       " '[3] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 4th edition, 2017.',\n",
       " '6',\n",
       " '\\x0c[4] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.',\n",
       " 'CoRR, abs/1712.03141, 2017.',\n",
       " '[5] Michael Br¨uckner and Tobias Scheﬀer. Stackelberg games for adversarial prediction problems. In Pro-\\nceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,\\npages 547–555. ACM, 2011.',\n",
       " '[6] S´ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed',\n",
       " 'bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.',\n",
       " '[7] Qi-Zhi Cai, Min Du, Chang Liu, and Dawn Song. Curriculum adversarial training.',\n",
       " 'In The 27th',\n",
       " 'International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2018.',\n",
       " '[8] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on\\ngraph structured data. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International\\nConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1115–\\n1124, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018. PMLR.',\n",
       " '[9] Yang Fan, Fei Tian, Tao Qin, and Tie-Yan Liu. Learning to teach. In ICLR, 2018.',\n",
       " '[10] Terry L Friesz. Dynamic optimization and diﬀerential games, volume 135. Springer Science & Business',\n",
       " 'Media, 2010.',\n",
       " '[11] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on',\n",
       " 'neural network policies. arXiv, 2017.',\n",
       " '[12] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Ma-\\nnipulating machine learning: Poisoning attacks and countermeasures for regression learning. The 39th\\nIEEE Symposium on Security and Privacy, 2018.',\n",
       " '[13] Anthony D. Joseph, Blaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar. Adversarial Machine',\n",
       " 'Learning. Cambridge University Press, 2018. in press.',\n",
       " '[14] Kwang-Sung Jun, Lihong Li, Yuzhe Ma, and Xiaojin Zhu. Adversarial attacks on stochastic bandits.',\n",
       " 'In Advances in Neural Information Processing Systems (NIPS), 2018.',\n",
       " '[15] L. Lessard, X. Zhang, and X. Zhu. An Optimal Control Approach to Sequential Machine Teaching.',\n",
       " 'ArXiv e-prints, October 2018.',\n",
       " '[16] Bo Li and Yevgeniy Vorobeychik. Scalable Optimization of Randomized Operational Decisions in Ad-\\nversarial Classiﬁcation Settings. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the\\nEighteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 38 of Proceedings\\nof Machine Learning Research, pages 599–607, San Diego, California, USA, 09–12 May 2015. PMLR.',\n",
       " '[17] Daniel Liberzon. Calculus of variations and optimal control theory: A concise introduction. Princeton',\n",
       " 'University Press, 2011.',\n",
       " 'and Le Song.\\n2149–2158, 2017.',\n",
       " '[18] Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg,\\nIn International Conference on Machine Learning, pages',\n",
       " 'Iterative machine teaching.',\n",
       " '[19] Weiyang Liu, Bo Dai, Xingguo Li, Zhen Liu, James M. Rehg, and Le Song. Towards black-box iterative\\nmachine teaching. In ICML, volume 80 of JMLR Workshop and Conference Proceedings, pages 3147–\\n3155. JMLR.org, 2018.',\n",
       " '[20] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD',\n",
       " 'international conference on Knowledge discovery in data mining, pages 641–647. ACM, 2005.',\n",
       " '7',\n",
       " '\\x0c[21] Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine',\n",
       " 'learners. In The Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015.',\n",
       " '[22] Kaustubh Patil, Xiaojin Zhu, Lukasz Kopec, and Bradley Love. Optimal teaching for limited-capacity',\n",
       " 'human learners. In Advances in Neural Information Processing Systems (NIPS), 2014.',\n",
       " '[23] B. Recht. A Tour of Reinforcement Learning: The View from Continuous Control. ArXiv e-prints, June',\n",
       " '2018.',\n",
       " '[24] Ayon Sen, Purav Patel, Martina A. Rau, Blake Mason, Robert Nowak, Timothy T. Rogers, and Xiaojin\\nZhu. Machine beats human at sequencing visuals for perceptual-ﬂuency practice. In Educational Data\\nMining, 2018.',\n",
       " '[25] Emanuel Todorov. Optimal control theory. Bayesian brain: probabilistic approaches to neural coding,',\n",
       " 'pages 269–298, 2006.',\n",
       " '[26] Yevgeniy Vorobeychik and Murat Kantarcioglu. Adversarial machine learning. Synthesis Lectures on',\n",
       " 'Artiﬁcial Intelligence and Machine Learning, 12(3):1–169, 2018.',\n",
       " '[27] Xiaojin Zhu. Machine teaching: an inverse problem to machine learning and an approach toward optimal\\neducation. In The Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence (AAAI “Blue Sky” Senior\\nMember Presentation Track), 2015.',\n",
       " '[28] Xiaojin Zhu, Ji Liu, and Manuel Lopes. No learner left behind: On the complexity of teaching multiple\\nlearners simultaneously. In The 26th International Joint Conference on Artiﬁcial Intelligence (IJCAI),\\n2017.',\n",
       " '[29] Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N. Raﬀerty. An Overview of Machine Teaching.',\n",
       " 'ArXiv e-prints, January 2018. https://arxiv.org/abs/1801.05927.',\n",
       " '8',\n",
       " '\\x0c']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0c',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c3.2 Test-Time Attack',\n",
       " 'Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,',\n",
       " 'min',\n",
       " '′',\n",
       " 'x',\n",
       " 's.t.',\n",
       " 'distance(x, x′)',\n",
       " 'h(x) 6= h(y).',\n",
       " '(9)',\n",
       " 'The distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm kx − x′kp.',\n",
       " 'One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:',\n",
       " '• Let the initial state x0 = x be the clean image.',\n",
       " '• The adversary’s control input u0 is the vector of pixel value changes.',\n",
       " '• The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid',\n",
       " 'pixel values (assumed to be normalized in [0, 1]).',\n",
       " '• The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0.',\n",
       " '• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).',\n",
       " '• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is',\n",
       " 'only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.',\n",
       " 'With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).',\n",
       " 'This control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.',\n",
       " '3.3 Defense Against Test-Time Attack by Adversarial Training',\n",
       " 'Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):',\n",
       " 'This is an uncountable number of constraints.\\nSVMs, but impractical otherwise.',\n",
       " '∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y.',\n",
       " '(10)',\n",
       " 'It is relatively easy to enforce for linear learners such as',\n",
       " 'Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.',\n",
       " 'It should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:',\n",
       " '4',\n",
       " '\\x0c',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c3.2 Test-Time Attack',\n",
       " 'Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,',\n",
       " 'min',\n",
       " '′',\n",
       " 'x',\n",
       " 's.t.',\n",
       " 'distance(x, x′)',\n",
       " 'h(x) 6= h(y).',\n",
       " '(9)',\n",
       " 'The distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm kx − x′kp.',\n",
       " 'One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:',\n",
       " '• Let the initial state x0 = x be the clean image.',\n",
       " '• The adversary’s control input u0 is the vector of pixel value changes.',\n",
       " '• The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid',\n",
       " 'pixel values (assumed to be normalized in [0, 1]).',\n",
       " '• The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0.',\n",
       " '• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).',\n",
       " '• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is',\n",
       " 'only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.',\n",
       " 'With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).',\n",
       " 'This control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.',\n",
       " '3.3 Defense Against Test-Time Attack by Adversarial Training',\n",
       " 'Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):',\n",
       " 'This is an uncountable number of constraints.\\nSVMs, but impractical otherwise.',\n",
       " '∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y.',\n",
       " '(10)',\n",
       " 'It is relatively easy to enforce for linear learners such as',\n",
       " 'Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.',\n",
       " 'It should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:',\n",
       " '4',\n",
       " '\\x0c• The state is the model ht. Initially h0 can be the model trained on the original training data.',\n",
       " '• The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X× y.',\n",
       " '• The dynamics ht+1 = f (ht, ut) is one-step update of the model, e.g. by back-propagation.',\n",
       " '• The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running',\n",
       " '• The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the',\n",
       " 'cost sums up to k).',\n",
       " 'original training data.',\n",
       " 'Of course, the resulting control problem (4) does not directly utilize adversarial examples. One way to\\nincorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on\\nht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks.',\n",
       " '3.4 Adversarial Reward Shaping',\n",
       " 'When adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce-\\nment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary.\\nThe adversary may do so by manipulating the rewards and the states experienced by the learner [11, 14].',\n",
       " 'To simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit,\\nbecause this does not involve deception through perceived states. To review, in stochastic multi-armed bandit\\nthe learner at iteration t chooses one of k arms, denoted by It ∈ [k], to pull according to some strategy [6].\\nFor example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm',\n",
       " 'It ∈ argmaxi∈[k] ˆµi,Ti(t−1) + ψ∗−1(cid:18) α log t',\n",
       " 'Ti(t − 1)(cid:19)',\n",
       " 'where Ti(t − 1) is the number of times arm i has been pulled up to time t − 1, ˆµi,Ti(t−1) is the empirical\\nmean of arm i so far, and ψ∗ is the dual of a convex function ψ. The environment generates a stochastic\\nreward rIt ∼ νIt . The learner updates its estimate of the pulled arm:',\n",
       " '(11)',\n",
       " '(12)',\n",
       " 'ˆµIt,TIt (t) =',\n",
       " 'ˆµIt,TIt (t−1)TIt (t − 1) + rIt',\n",
       " 'TIt(t − 1) + 1',\n",
       " 'which in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the\\nt=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit',\n",
       " 'pseudo-regret T µmax − EPT',\n",
       " 'strategies oﬀer upper bounds on the pseudo-regret.',\n",
       " 'With adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the',\n",
       " 'environmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into',\n",
       " 'with some ut ∈ R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal\\nreward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may\\nwant the learner to frequently pull a particular target arm i∗ ∈ [k]. It should be noted that the adversary’s\\ngoal may not be the exact opposite of the learner’s goal: the target arm i∗ is not necessarily the one with\\nthe worst mean reward, and the adversary may not seek pseudo-regret maximization.',\n",
       " 'Adversarial reward shaping can be formulated as stochastic optimal control:',\n",
       " '• The state st, now called control state to avoid confusion with the Markov Decision Process states',\n",
       " 'experienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t:',\n",
       " 'st = (T1(t − 1), ˆµ1,T1(t−1), . . . , Tk(t − 1), ˆµk,Tk(t−1), It).',\n",
       " 'rIt + ut',\n",
       " '5',\n",
       " '\\x0c',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c3.2 Test-Time Attack',\n",
       " 'Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,',\n",
       " 'min',\n",
       " '′',\n",
       " 'x',\n",
       " 's.t.',\n",
       " 'distance(x, x′)',\n",
       " 'h(x) 6= h(y).',\n",
       " '(9)',\n",
       " 'The distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm kx − x′kp.',\n",
       " 'One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:',\n",
       " '• Let the initial state x0 = x be the clean image.',\n",
       " '• The adversary’s control input u0 is the vector of pixel value changes.',\n",
       " '• The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid',\n",
       " 'pixel values (assumed to be normalized in [0, 1]).',\n",
       " '• The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0.',\n",
       " '• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).',\n",
       " '• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is',\n",
       " 'only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.',\n",
       " 'With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).',\n",
       " 'This control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.',\n",
       " '3.3 Defense Against Test-Time Attack by Adversarial Training',\n",
       " 'Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):',\n",
       " 'This is an uncountable number of constraints.\\nSVMs, but impractical otherwise.',\n",
       " '∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y.',\n",
       " '(10)',\n",
       " 'It is relatively easy to enforce for linear learners such as',\n",
       " 'Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.',\n",
       " 'It should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:',\n",
       " '4',\n",
       " '\\x0c• The state is the model ht. Initially h0 can be the model trained on the original training data.',\n",
       " '• The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X× y.',\n",
       " '• The dynamics ht+1 = f (ht, ut) is one-step update of the model, e.g. by back-propagation.',\n",
       " '• The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running',\n",
       " '• The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the',\n",
       " 'cost sums up to k).',\n",
       " 'original training data.',\n",
       " 'Of course, the resulting control problem (4) does not directly utilize adversarial examples. One way to\\nincorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on\\nht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks.',\n",
       " '3.4 Adversarial Reward Shaping',\n",
       " 'When adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce-\\nment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary.\\nThe adversary may do so by manipulating the rewards and the states experienced by the learner [11, 14].',\n",
       " 'To simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit,\\nbecause this does not involve deception through perceived states. To review, in stochastic multi-armed bandit\\nthe learner at iteration t chooses one of k arms, denoted by It ∈ [k], to pull according to some strategy [6].\\nFor example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm',\n",
       " 'It ∈ argmaxi∈[k] ˆµi,Ti(t−1) + ψ∗−1(cid:18) α log t',\n",
       " 'Ti(t − 1)(cid:19)',\n",
       " 'where Ti(t − 1) is the number of times arm i has been pulled up to time t − 1, ˆµi,Ti(t−1) is the empirical\\nmean of arm i so far, and ψ∗ is the dual of a convex function ψ. The environment generates a stochastic\\nreward rIt ∼ νIt . The learner updates its estimate of the pulled arm:',\n",
       " '(11)',\n",
       " '(12)',\n",
       " 'ˆµIt,TIt (t) =',\n",
       " 'ˆµIt,TIt (t−1)TIt (t − 1) + rIt',\n",
       " 'TIt(t − 1) + 1',\n",
       " 'which in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the\\nt=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit',\n",
       " 'pseudo-regret T µmax − EPT',\n",
       " 'strategies oﬀer upper bounds on the pseudo-regret.',\n",
       " 'With adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the',\n",
       " 'environmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into',\n",
       " 'with some ut ∈ R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal\\nreward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may\\nwant the learner to frequently pull a particular target arm i∗ ∈ [k]. It should be noted that the adversary’s\\ngoal may not be the exact opposite of the learner’s goal: the target arm i∗ is not necessarily the one with\\nthe worst mean reward, and the adversary may not seek pseudo-regret maximization.',\n",
       " 'Adversarial reward shaping can be formulated as stochastic optimal control:',\n",
       " '• The state st, now called control state to avoid confusion with the Markov Decision Process states',\n",
       " 'experienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t:',\n",
       " 'st = (T1(t − 1), ˆµ1,T1(t−1), . . . , Tk(t − 1), ˆµk,Tk(t−1), It).',\n",
       " 'rIt + ut',\n",
       " '5',\n",
       " '\\x0c• The control input is ut ∈ Ut with Ut = R in the unconstrained shaping case, or the appropriate Ut if',\n",
       " 'the rewards must be binary, for example.',\n",
       " '• The dynamics st+1 = f (st, ut) is straightforward via empirical mean update (12), TIt increment, and',\n",
       " 'new arm choice (11).',\n",
       " 't. For instance,',\n",
       " '• The adversary’s running cost gt(st, ut) reﬂects shaping eﬀort and target arm achievement in iteration',\n",
       " 'gt(st, ut) = u2',\n",
       " 't + Iλ[It 6= i∗].',\n",
       " '(13)',\n",
       " 'where λ > 0 is a trade oﬀ parameter.',\n",
       " '• There is not necessarily a time horizon T or a terminal cost gT (sT ).',\n",
       " 'The control state is stochastic due to the stochastic reward rIt entering through (12).',\n",
       " '4 Advantages of the Optimal Control View',\n",
       " 'There are a number of potential beneﬁts in taking the optimal control view:',\n",
       " '• It oﬀers a uniﬁed conceptual framework for adversarial machine learning;',\n",
       " '• The optimal control literature provides eﬃcient solutions when the dynamics f is known and one can',\n",
       " 'take the continuous limit to solve the diﬀerential equations [15];',\n",
       " '• Reinforcement learning, either model-based with coarse system identiﬁcation or model-free policy it-\\neration, allows approximate optimal control when f is unknown, as long as the adversary can probe\\nthe dynamics [8, 9];',\n",
       " '• A generic defense strategy may be to limit the controllability the adversary has over the learner.',\n",
       " '• I mention in passing that the optimal control view applies equally to machine teaching [27, 29], and',\n",
       " 'thus extends to the application of personalized education [22, 24].',\n",
       " 'I need to point out some limitations:',\n",
       " '• Having a uniﬁed optimal control view does not automatically produce eﬃcient solutions to the control\\nproblem (4). For adversarial machine learning applications the dynamics f is usually highly nonlinear\\nand complex. Furthermore, in graybox and blackbox attack settings f is not fully known to the\\nattacker. They aﬀect the complexity in ﬁnding an optimal control.',\n",
       " '• The adversarial learning setting is largely non-game theoretic, though there are exceptions [5, 16].',\n",
       " 'These problems call for future research from both machine learning and control communities.',\n",
       " 'Acknowledgments. I acknowledge funding NSF 1837132, 1545481, 1704117, 1623605, 1561512, and',\n",
       " 'the MADLab AF Center of Excellence FA9550-18-1-0166.',\n",
       " 'References',\n",
       " '[1] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In',\n",
       " 'The Thirtieth AAAI Conference on Artiﬁcial Intelligence (AAAI-16), 2016.',\n",
       " '[2] Michael Athans and Peter L Falb. Optimal control: An introduction to the theory and its applications.',\n",
       " 'Courier Corporation, 2013.',\n",
       " '[3] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 4th edition, 2017.',\n",
       " '6',\n",
       " '\\x0c',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c3.2 Test-Time Attack',\n",
       " 'Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,',\n",
       " 'min',\n",
       " '′',\n",
       " 'x',\n",
       " 's.t.',\n",
       " 'distance(x, x′)',\n",
       " 'h(x) 6= h(y).',\n",
       " '(9)',\n",
       " 'The distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm kx − x′kp.',\n",
       " 'One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:',\n",
       " '• Let the initial state x0 = x be the clean image.',\n",
       " '• The adversary’s control input u0 is the vector of pixel value changes.',\n",
       " '• The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid',\n",
       " 'pixel values (assumed to be normalized in [0, 1]).',\n",
       " '• The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0.',\n",
       " '• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).',\n",
       " '• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is',\n",
       " 'only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.',\n",
       " 'With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).',\n",
       " 'This control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.',\n",
       " '3.3 Defense Against Test-Time Attack by Adversarial Training',\n",
       " 'Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):',\n",
       " 'This is an uncountable number of constraints.\\nSVMs, but impractical otherwise.',\n",
       " '∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y.',\n",
       " '(10)',\n",
       " 'It is relatively easy to enforce for linear learners such as',\n",
       " 'Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.',\n",
       " 'It should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:',\n",
       " '4',\n",
       " '\\x0c• The state is the model ht. Initially h0 can be the model trained on the original training data.',\n",
       " '• The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X× y.',\n",
       " '• The dynamics ht+1 = f (ht, ut) is one-step update of the model, e.g. by back-propagation.',\n",
       " '• The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running',\n",
       " '• The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the',\n",
       " 'cost sums up to k).',\n",
       " 'original training data.',\n",
       " 'Of course, the resulting control problem (4) does not directly utilize adversarial examples. One way to\\nincorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on\\nht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks.',\n",
       " '3.4 Adversarial Reward Shaping',\n",
       " 'When adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce-\\nment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary.\\nThe adversary may do so by manipulating the rewards and the states experienced by the learner [11, 14].',\n",
       " 'To simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit,\\nbecause this does not involve deception through perceived states. To review, in stochastic multi-armed bandit\\nthe learner at iteration t chooses one of k arms, denoted by It ∈ [k], to pull according to some strategy [6].\\nFor example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm',\n",
       " 'It ∈ argmaxi∈[k] ˆµi,Ti(t−1) + ψ∗−1(cid:18) α log t',\n",
       " 'Ti(t − 1)(cid:19)',\n",
       " 'where Ti(t − 1) is the number of times arm i has been pulled up to time t − 1, ˆµi,Ti(t−1) is the empirical\\nmean of arm i so far, and ψ∗ is the dual of a convex function ψ. The environment generates a stochastic\\nreward rIt ∼ νIt . The learner updates its estimate of the pulled arm:',\n",
       " '(11)',\n",
       " '(12)',\n",
       " 'ˆµIt,TIt (t) =',\n",
       " 'ˆµIt,TIt (t−1)TIt (t − 1) + rIt',\n",
       " 'TIt(t − 1) + 1',\n",
       " 'which in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the\\nt=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit',\n",
       " 'pseudo-regret T µmax − EPT',\n",
       " 'strategies oﬀer upper bounds on the pseudo-regret.',\n",
       " 'With adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the',\n",
       " 'environmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into',\n",
       " 'with some ut ∈ R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal\\nreward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may\\nwant the learner to frequently pull a particular target arm i∗ ∈ [k]. It should be noted that the adversary’s\\ngoal may not be the exact opposite of the learner’s goal: the target arm i∗ is not necessarily the one with\\nthe worst mean reward, and the adversary may not seek pseudo-regret maximization.',\n",
       " 'Adversarial reward shaping can be formulated as stochastic optimal control:',\n",
       " '• The state st, now called control state to avoid confusion with the Markov Decision Process states',\n",
       " 'experienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t:',\n",
       " 'st = (T1(t − 1), ˆµ1,T1(t−1), . . . , Tk(t − 1), ˆµk,Tk(t−1), It).',\n",
       " 'rIt + ut',\n",
       " '5',\n",
       " '\\x0c• The control input is ut ∈ Ut with Ut = R in the unconstrained shaping case, or the appropriate Ut if',\n",
       " 'the rewards must be binary, for example.',\n",
       " '• The dynamics st+1 = f (st, ut) is straightforward via empirical mean update (12), TIt increment, and',\n",
       " 'new arm choice (11).',\n",
       " 't. For instance,',\n",
       " '• The adversary’s running cost gt(st, ut) reﬂects shaping eﬀort and target arm achievement in iteration',\n",
       " 'gt(st, ut) = u2',\n",
       " 't + Iλ[It 6= i∗].',\n",
       " '(13)',\n",
       " 'where λ > 0 is a trade oﬀ parameter.',\n",
       " '• There is not necessarily a time horizon T or a terminal cost gT (sT ).',\n",
       " 'The control state is stochastic due to the stochastic reward rIt entering through (12).',\n",
       " '4 Advantages of the Optimal Control View',\n",
       " 'There are a number of potential beneﬁts in taking the optimal control view:',\n",
       " '• It oﬀers a uniﬁed conceptual framework for adversarial machine learning;',\n",
       " '• The optimal control literature provides eﬃcient solutions when the dynamics f is known and one can',\n",
       " 'take the continuous limit to solve the diﬀerential equations [15];',\n",
       " '• Reinforcement learning, either model-based with coarse system identiﬁcation or model-free policy it-\\neration, allows approximate optimal control when f is unknown, as long as the adversary can probe\\nthe dynamics [8, 9];',\n",
       " '• A generic defense strategy may be to limit the controllability the adversary has over the learner.',\n",
       " '• I mention in passing that the optimal control view applies equally to machine teaching [27, 29], and',\n",
       " 'thus extends to the application of personalized education [22, 24].',\n",
       " 'I need to point out some limitations:',\n",
       " '• Having a uniﬁed optimal control view does not automatically produce eﬃcient solutions to the control\\nproblem (4). For adversarial machine learning applications the dynamics f is usually highly nonlinear\\nand complex. Furthermore, in graybox and blackbox attack settings f is not fully known to the\\nattacker. They aﬀect the complexity in ﬁnding an optimal control.',\n",
       " '• The adversarial learning setting is largely non-game theoretic, though there are exceptions [5, 16].',\n",
       " 'These problems call for future research from both machine learning and control communities.',\n",
       " 'Acknowledgments. I acknowledge funding NSF 1837132, 1545481, 1704117, 1623605, 1561512, and',\n",
       " 'the MADLab AF Center of Excellence FA9550-18-1-0166.',\n",
       " 'References',\n",
       " '[1] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In',\n",
       " 'The Thirtieth AAAI Conference on Artiﬁcial Intelligence (AAAI-16), 2016.',\n",
       " '[2] Michael Athans and Peter L Falb. Optimal control: An introduction to the theory and its applications.',\n",
       " 'Courier Corporation, 2013.',\n",
       " '[3] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 4th edition, 2017.',\n",
       " '6',\n",
       " '\\x0c[4] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.',\n",
       " 'CoRR, abs/1712.03141, 2017.',\n",
       " '[5] Michael Br¨uckner and Tobias Scheﬀer. Stackelberg games for adversarial prediction problems. In Pro-\\nceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,\\npages 547–555. ACM, 2011.',\n",
       " '[6] S´ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed',\n",
       " 'bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.',\n",
       " '[7] Qi-Zhi Cai, Min Du, Chang Liu, and Dawn Song. Curriculum adversarial training.',\n",
       " 'In The 27th',\n",
       " 'International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2018.',\n",
       " '[8] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on\\ngraph structured data. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International\\nConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1115–\\n1124, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018. PMLR.',\n",
       " '[9] Yang Fan, Fei Tian, Tao Qin, and Tie-Yan Liu. Learning to teach. In ICLR, 2018.',\n",
       " '[10] Terry L Friesz. Dynamic optimization and diﬀerential games, volume 135. Springer Science & Business',\n",
       " 'Media, 2010.',\n",
       " '[11] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on',\n",
       " 'neural network policies. arXiv, 2017.',\n",
       " '[12] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Ma-\\nnipulating machine learning: Poisoning attacks and countermeasures for regression learning. The 39th\\nIEEE Symposium on Security and Privacy, 2018.',\n",
       " '[13] Anthony D. Joseph, Blaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar. Adversarial Machine',\n",
       " 'Learning. Cambridge University Press, 2018. in press.',\n",
       " '[14] Kwang-Sung Jun, Lihong Li, Yuzhe Ma, and Xiaojin Zhu. Adversarial attacks on stochastic bandits.',\n",
       " 'In Advances in Neural Information Processing Systems (NIPS), 2018.',\n",
       " '[15] L. Lessard, X. Zhang, and X. Zhu. An Optimal Control Approach to Sequential Machine Teaching.',\n",
       " 'ArXiv e-prints, October 2018.',\n",
       " '[16] Bo Li and Yevgeniy Vorobeychik. Scalable Optimization of Randomized Operational Decisions in Ad-\\nversarial Classiﬁcation Settings. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the\\nEighteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 38 of Proceedings\\nof Machine Learning Research, pages 599–607, San Diego, California, USA, 09–12 May 2015. PMLR.',\n",
       " '[17] Daniel Liberzon. Calculus of variations and optimal control theory: A concise introduction. Princeton',\n",
       " 'University Press, 2011.',\n",
       " 'and Le Song.\\n2149–2158, 2017.',\n",
       " '[18] Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg,\\nIn International Conference on Machine Learning, pages',\n",
       " 'Iterative machine teaching.',\n",
       " '[19] Weiyang Liu, Bo Dai, Xingguo Li, Zhen Liu, James M. Rehg, and Le Song. Towards black-box iterative\\nmachine teaching. In ICML, volume 80 of JMLR Workshop and Conference Proceedings, pages 3147–\\n3155. JMLR.org, 2018.',\n",
       " '[20] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD',\n",
       " 'international conference on Knowledge discovery in data mining, pages 641–647. ACM, 2005.',\n",
       " '7',\n",
       " '\\x0c',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " '.',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0cMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs.\\nIn optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2, 10, 17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].',\n",
       " '3 Adversarial Machine Learning as Control',\n",
       " 'Now let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.',\n",
       " 'Unfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.',\n",
       " '3.1 Training-Data Poisoning',\n",
       " 'In training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.',\n",
       " '3.1.1 Batch Learner',\n",
       " 'At this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:',\n",
       " '• The state is the learner’s model h : X 7→ Y. For instance, for SVM h is the classiﬁer parametrized by',\n",
       " 'a weight vector w. I will use h and w interchangeably.',\n",
       " '• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.',\n",
       " '• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.',\n",
       " '• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this',\n",
       " 'would be empirical risk minimization with hinge loss ℓ() and a regularizer:',\n",
       " 'w1 = f (u0) ∈ argminw',\n",
       " 'ℓ(w, xi, yi) + λkwk2.',\n",
       " '(5)',\n",
       " 'The batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf () if it knows the form (5), ℓ(), and the value of λ.',\n",
       " '• The time horizon T = 1.',\n",
       " 'n',\n",
       " 'Xi=1',\n",
       " '2',\n",
       " '\\x0c• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.',\n",
       " 'This is typically deﬁned with respect to a given “clean” data set ˜u before poisoning in the form of',\n",
       " 'g0(u0) = distance(u0, ˜u).',\n",
       " '(6)',\n",
       " 'The running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.',\n",
       " '• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also',\n",
       " 'domain dependent. For example:',\n",
       " '– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 6= w∗]. Here Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.',\n",
       " '– If the adversary only needs the learner to get near w∗ then g1(w1) = kw1 − w∗k for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗ is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /∈ W∗] with the target set W∗ = {w : w⊤x∗ ≥ ǫ}. More generally,\\nW∗ can be a polytope deﬁned by multiple future classiﬁcation constraints.',\n",
       " 'With these deﬁnitions, the adversary’s one-step control problem (4) specializes to',\n",
       " 'min',\n",
       " 'u0',\n",
       " 's.t.',\n",
       " 'g1(w1) + g0(w0, u0)',\n",
       " 'w1 = f (w0, u0)',\n",
       " '(7)',\n",
       " 'Unsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f ), a well-known formulation for training-data poi-\\nsoning [12, 21].',\n",
       " '3.1.2 Sequential Learner',\n",
       " 'The adversary performs classic discrete-time control if the learner is sequential:',\n",
       " '• The learner starts from an initial model w0, which is the initial state.',\n",
       " '• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .',\n",
       " '• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform',\n",
       " 'one step of gradient descent:',\n",
       " 'wt+1 = f (wt, ut) = wt − ηt∇ℓ(wt, xt, yt).',\n",
       " '(8)',\n",
       " '• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change kut − ˜utk with respect to a “clean” reference training sequence\\n˜u. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.',\n",
       " '• The adversary’s terminal cost gT (wT ) is the same as in the batch case.',\n",
       " 'The problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1, 18, 19].',\n",
       " '3',\n",
       " '\\x0c3.2 Test-Time Attack',\n",
       " 'Test-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→ Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,',\n",
       " 'min',\n",
       " '′',\n",
       " 'x',\n",
       " 's.t.',\n",
       " 'distance(x, x′)',\n",
       " 'h(x) 6= h(y).',\n",
       " '(9)',\n",
       " 'The distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm kx − x′kp.',\n",
       " 'One way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:',\n",
       " '• Let the initial state x0 = x be the clean image.',\n",
       " '• The adversary’s control input u0 is the vector of pixel value changes.',\n",
       " '• The control constraint set is U0 = {u : x0 + u ∈ [0, 1]d} to ensure that the modiﬁed image has valid',\n",
       " 'pixel values (assumed to be normalized in [0, 1]).',\n",
       " '• The dynamical system is trivially vector addition: x1 = f (x0, u0) = x0 + u0.',\n",
       " '• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).',\n",
       " '• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is',\n",
       " 'only used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.',\n",
       " 'With these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).',\n",
       " 'This control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.',\n",
       " '3.3 Defense Against Test-Time Attack by Adversarial Training',\n",
       " 'Some defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):',\n",
       " 'This is an uncountable number of constraints.\\nSVMs, but impractical otherwise.',\n",
       " '∀x′ : (kx′ − xkp ≤ ǫ) ⇒ h(x′) = y.',\n",
       " '(10)',\n",
       " 'It is relatively easy to enforce for linear learners such as',\n",
       " 'Adversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that kx(1) − xkp ≤ ǫ but h(x(1)) 6= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.',\n",
       " 'It should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:',\n",
       " '4',\n",
       " '\\x0c• The state is the model ht. Initially h0 can be the model trained on the original training data.',\n",
       " '• The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X× y.',\n",
       " '• The dynamics ht+1 = f (ht, ut) is one-step update of the model, e.g. by back-propagation.',\n",
       " '• The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running',\n",
       " '• The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the',\n",
       " 'cost sums up to k).',\n",
       " 'original training data.',\n",
       " 'Of course, the resulting control problem (4) does not directly utilize adversarial examples. One way to\\nincorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on\\nht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks.',\n",
       " '3.4 Adversarial Reward Shaping',\n",
       " 'When adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce-\\nment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary.\\nThe adversary may do so by manipulating the rewards and the states experienced by the learner [11, 14].',\n",
       " 'To simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit,\\nbecause this does not involve deception through perceived states. To review, in stochastic multi-armed bandit\\nthe learner at iteration t chooses one of k arms, denoted by It ∈ [k], to pull according to some strategy [6].\\nFor example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm',\n",
       " 'It ∈ argmaxi∈[k] ˆµi,Ti(t−1) + ψ∗−1(cid:18) α log t',\n",
       " 'Ti(t − 1)(cid:19)',\n",
       " 'where Ti(t − 1) is the number of times arm i has been pulled up to time t − 1, ˆµi,Ti(t−1) is the empirical\\nmean of arm i so far, and ψ∗ is the dual of a convex function ψ. The environment generates a stochastic\\nreward rIt ∼ νIt . The learner updates its estimate of the pulled arm:',\n",
       " '(11)',\n",
       " '(12)',\n",
       " 'ˆµIt,TIt (t) =',\n",
       " 'ˆµIt,TIt (t−1)TIt (t − 1) + rIt',\n",
       " 'TIt(t − 1) + 1',\n",
       " 'which in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the\\nt=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit',\n",
       " 'pseudo-regret T µmax − EPT',\n",
       " 'strategies oﬀer upper bounds on the pseudo-regret.',\n",
       " 'With adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the',\n",
       " 'environmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into',\n",
       " 'with some ut ∈ R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal\\nreward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may\\nwant the learner to frequently pull a particular target arm i∗ ∈ [k]. It should be noted that the adversary’s\\ngoal may not be the exact opposite of the learner’s goal: the target arm i∗ is not necessarily the one with\\nthe worst mean reward, and the adversary may not seek pseudo-regret maximization.',\n",
       " 'Adversarial reward shaping can be formulated as stochastic optimal control:',\n",
       " '• The state st, now called control state to avoid confusion with the Markov Decision Process states',\n",
       " 'experienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t:',\n",
       " 'st = (T1(t − 1), ˆµ1,T1(t−1), . . . , Tk(t − 1), ˆµk,Tk(t−1), It).',\n",
       " 'rIt + ut',\n",
       " '5',\n",
       " '\\x0c• The control input is ut ∈ Ut with Ut = R in the unconstrained shaping case, or the appropriate Ut if',\n",
       " 'the rewards must be binary, for example.',\n",
       " '• The dynamics st+1 = f (st, ut) is straightforward via empirical mean update (12), TIt increment, and',\n",
       " 'new arm choice (11).',\n",
       " 't. For instance,',\n",
       " '• The adversary’s running cost gt(st, ut) reﬂects shaping eﬀort and target arm achievement in iteration',\n",
       " 'gt(st, ut) = u2',\n",
       " 't + Iλ[It 6= i∗].',\n",
       " '(13)',\n",
       " 'where λ > 0 is a trade oﬀ parameter.',\n",
       " '• There is not necessarily a time horizon T or a terminal cost gT (sT ).',\n",
       " 'The control state is stochastic due to the stochastic reward rIt entering through (12).',\n",
       " '4 Advantages of the Optimal Control View',\n",
       " 'There are a number of potential beneﬁts in taking the optimal control view:',\n",
       " '• It oﬀers a uniﬁed conceptual framework for adversarial machine learning;',\n",
       " '• The optimal control literature provides eﬃcient solutions when the dynamics f is known and one can',\n",
       " 'take the continuous limit to solve the diﬀerential equations [15];',\n",
       " '• Reinforcement learning, either model-based with coarse system identiﬁcation or model-free policy it-\\neration, allows approximate optimal control when f is unknown, as long as the adversary can probe\\nthe dynamics [8, 9];',\n",
       " '• A generic defense strategy may be to limit the controllability the adversary has over the learner.',\n",
       " '• I mention in passing that the optimal control view applies equally to machine teaching [27, 29], and',\n",
       " 'thus extends to the application of personalized education [22, 24].',\n",
       " 'I need to point out some limitations:',\n",
       " '• Having a uniﬁed optimal control view does not automatically produce eﬃcient solutions to the control\\nproblem (4). For adversarial machine learning applications the dynamics f is usually highly nonlinear\\nand complex. Furthermore, in graybox and blackbox attack settings f is not fully known to the\\nattacker. They aﬀect the complexity in ﬁnding an optimal control.',\n",
       " '• The adversarial learning setting is largely non-game theoretic, though there are exceptions [5, 16].',\n",
       " 'These problems call for future research from both machine learning and control communities.',\n",
       " 'Acknowledgments. I acknowledge funding NSF 1837132, 1545481, 1704117, 1623605, 1561512, and',\n",
       " 'the MADLab AF Center of Excellence FA9550-18-1-0166.',\n",
       " 'References',\n",
       " '[1] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In',\n",
       " 'The Thirtieth AAAI Conference on Artiﬁcial Intelligence (AAAI-16), 2016.',\n",
       " '[2] Michael Athans and Peter L Falb. Optimal control: An introduction to the theory and its applications.',\n",
       " 'Courier Corporation, 2013.',\n",
       " '[3] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 4th edition, 2017.',\n",
       " '6',\n",
       " '\\x0c[4] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.',\n",
       " 'CoRR, abs/1712.03141, 2017.',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"abstract\" header does not exist, use this block to extract abstract and keywords\n",
    "if not abstract_string:\n",
    "    stop = 0\n",
    "    # find break point to stop at introduction\n",
    "    while True:\n",
    "        if \"introduction\" in text[stop].lower():\n",
    "            break\n",
    "        else:\n",
    "            stop += 1\n",
    "    cnt = 0\n",
    "    # iterate through each line up to introduction\n",
    "    while cnt < stop:\n",
    "        # if keywords, put this in keywords variable\n",
    "        if \"keywords\" in text[cnt].lower() or \"key-words\" in text[cnt].lower() or \"keyword\" in text[cnt].lower() or \"key word\" in text[cnt].lower():\n",
    "            if len(text[cnt].split()) < 2:\n",
    "                cnt += 1\n",
    "            keywords_string += text[cnt]\n",
    "        # if a number or a short piece of redundant text\n",
    "        elif len(text[cnt].split()) < 2:\n",
    "            pass\n",
    "        # if ocr has picked up annoying numebrs along side with many \"\\n\"\n",
    "        elif text[cnt].count(\"\\n\") >= 3:\n",
    "            dummy = text[cnt].split(\"\\n\")\n",
    "            dummy_cnt = 0\n",
    "            for d in dummy:\n",
    "                if len(d)>1:\n",
    "                    dummy_cnt += 1\n",
    "            if dummy_cnt > 2:\n",
    "                abstract_string += text[cnt]+\" \" \n",
    "        else:\n",
    "            abstract_string += text[cnt]+\" \"\n",
    "        cnt += 1\n",
    "# block to clean up keywords if it has appended extra text to the start\n",
    "keywords_string_2 = \"\"\n",
    "if \"\\n\" in keywords_string:\n",
    "    keywords_dummy = keywords_string.split(\"\\n\")\n",
    "    if \"keywords\" in keywords_dummy[0].lower() or \"key-words\" in keywords_dummy[0].lower() or \"keyword\" in keywords_dummy[0].lower() or \"key word\" in keywords_dummy[0].lower():\n",
    "        pass\n",
    "    else:\n",
    "        cnt = 0\n",
    "        while cnt < len(keywords_dummy):\n",
    "            if \"keywords\" in keywords_dummy[cnt].lower() or \"key-words\" in keywords_dummy[cnt].lower() or \"keyword\" in keywords_dummy[cnt].lower() or \"key word\" in keywords_dummy[cnt].lower():\n",
    "                if len(keywords_dummy[cnt].split()) < 2:\n",
    "                    cnt += 1\n",
    "                keywords_string_2 += keywords_dummy[cnt]\n",
    "            cnt += 1\n",
    "        keywords_string = keywords_string_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8\\n1\\n0\\n2',\n",
       " ' ',\n",
       " 'v\\no\\nN\\n1\\n1',\n",
       " ' ',\n",
       " ' \\n \\n]',\n",
       " 'G\\nL\\n.\\ns\\nc\\n[\\n \\n ',\n",
       " '1\\nv\\n2\\n2\\n4\\n4\\n0',\n",
       " '.',\n",
       " '1\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na',\n",
       " 'An Optimal Control View of Adversarial Machine Learning',\n",
       " 'Department of Computer Sciences, University of Wisconsin-Madison',\n",
       " 'Xiaojin Zhu',\n",
       " 'Abstract',\n",
       " 'I describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.',\n",
       " '1 Adversarial Machine Learning is not Machine Learning',\n",
       " 'Machine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3, 25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.',\n",
       " '2 Optimal Control',\n",
       " 'I will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:',\n",
       " 'where xt ∈ Xt is the state of the system, ut ∈ Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T − 1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:',\n",
       " 'which deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:',\n",
       " 'xt+1 = f (xt, ut)',\n",
       " 'gt(xt, ut)',\n",
       " 'gT (xT )',\n",
       " 'which deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(3)',\n",
       " '(4)',\n",
       " 'min',\n",
       " 'u0...uT −1',\n",
       " 's.t.',\n",
       " 'gT (xT ) +',\n",
       " 'gt(xt, ut)',\n",
       " 'T −1',\n",
       " 'Xt=0',\n",
       " 'xt+1 = f (xt, ut), ut ∈ Ut, ∀t\\nx0 given',\n",
       " '1',\n",
       " '\\x0c']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
