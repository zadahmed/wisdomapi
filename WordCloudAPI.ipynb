{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import string\n",
    "import requests\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wisdomaiengine import pdfdocumentextracter, wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for pdfurl in [\"https://arxiv.org/pdf/2001.09903.pdf\", \"http://arxiv.org/pdf/1811.04422v1\",\n",
    "            \"https://arxiv.org/pdf/2001.09956\",\n",
    "            \"https://arxiv.org/pdf/2001.09412.pdf\",\n",
    "            \"http://arxiv.org/pdf/1411.6753v1\",\n",
    "            \"https://arxiv.org/pdf/2001.10393.pdf\"]:\n",
    "    corpus.append([pdfdocumentextracter(pdfurl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables and functions\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "extras = [\"et\", \"al\", \"le\", \"eg\"]\n",
    "for extra in extras:\n",
    "    stop.add(extra)\n",
    "    \n",
    "extras = [\"•\", \"−\"]\n",
    "for extra in extras:\n",
    "    exclude.add(extra)\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_processor(corpus):\n",
    "    all_text = ' '.join(i[0] for i in corpus)\n",
    "    formatted_all_text = all_text.lower()\n",
    "    formatted_all_text = re.sub(r'[^\\w\\s]',' ',formatted_all_text)\n",
    "    formatted_all_text = \" \".join(x for x in formatted_all_text.split() if x not in stop)\n",
    "    all_text_sent = all_text\n",
    "    # If there is no data\n",
    "    if not formatted_all_text or not all_text:\n",
    "        frequency = None\n",
    "        return frequency\n",
    "    # Otherwise\n",
    "    sentence_list = sent_tokenize(all_text_sent)\n",
    "    split_words = [f for f in formatted_all_text.split(\" \") if len(f) > 2]\n",
    "    frequency = pd.value_counts(split_words).reset_index()\n",
    "    frequency.columns = [\"words\", \"frequency\"]\n",
    "    frequency = frequency[frequency[\"words\"] != \"-\"]\n",
    "    frequency = frequency[frequency[\"words\"] != \"_\"]\n",
    "    maximum_frequency = max(frequency[\"frequency\"].values)\n",
    "    frequency[\"weighted_frequency\"] = frequency[\"frequency\"]/maximum_frequency\n",
    "    for i, word in enumerate(frequency[\"words\"]):\n",
    "        frequency.loc[i, 'idf'] = np.log(len(sentence_list)/len([x for x in sentence_list if word in x.lower()]))\n",
    "    for i, word in enumerate(frequency[\"words\"]):\n",
    "        try:\n",
    "            frequency.loc[i, 'lemmatized word'] = lemma.lemmatize(word)\n",
    "        except:\n",
    "            frequency.loc[i, 'lemmatized word'] = \" \"\n",
    "    frequency['tf_idf'] = frequency['frequency'] * frequency['idf']\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"neural network\"\n",
    "num_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if corpus:\n",
    "    # Important Words Modeling\n",
    "    frequency = frequency_processor(corpus)\n",
    "    important_words = []\n",
    "    if frequency is not None:\n",
    "        top_N = pd.DataFrame(frequency.groupby(\"lemmatized word\")[\"tf_idf\"].sum())\n",
    "        top_N = top_N.sort_values(by=[\"tf_idf\"], ascending=False)\n",
    "        split = search_term.split()\n",
    "        counter1=0\n",
    "        counter2=1\n",
    "        while counter2<(num_words+1):\n",
    "            word = top_N.index[counter1]\n",
    "            value = top_N.values[counter1]\n",
    "            thresh=0\n",
    "            for i in split:\n",
    "                if i in word:\n",
    "                    thresh+=1\n",
    "                else:\n",
    "                    pass\n",
    "            if thresh>0:\n",
    "                counter1+=1\n",
    "            else:\n",
    "                important_words.append([word, value[0]])\n",
    "                counter1+=1\n",
    "                counter2+=1\n",
    "        if important_words[0] == \"\":\n",
    "            important_words = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wisdomaiengine import pdfdocumentextracter, wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for pdfurl in [\"https://arxiv.org/pdf/2001.09903.pdf\"]:\n",
    "    corpus.append([pdfdocumentextracter(pdfurl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = wordcloud(\"neural network\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outburst  -  57.60532022606475\n",
      "ray  -  50.124016458243645\n",
      "emission  -  40.035536662232005\n",
      "optical  -  38.65305966459018\n",
      "aql  -  38.562574190707245\n",
      "accretion  -  34.09103986456174\n",
      "star  -  34.01852194787096\n",
      "kev  -  31.40874999400895\n",
      "lmxbs  -  29.2208913124815\n",
      "observation  -  27.12220296926123\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word[0], \" - \", word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
